# -*- coding: utf-8 -*-
"""CB001 - Chapter 12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sc-qV4K8DirzlS_1RF_FDOFgIkx9iIPd

# Polynomial Regression
"""

#Mount the google drive connection to our dataset
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd
df = pd.read_csv('/content/drive/My Drive/AI/datasets/laptop_pricing_dataset.csv')

df.head()

df.info()

df.describe()

df.drop("Unnamed: 0", axis=1, inplace=True)

"""Visualize the data"""

import matplotlib.pyplot as plt
import seaborn as sns

def plot_charts(df):
  # Numeric features
  numeric_features = [col for col in df.columns if df[col].dtype in ['int64', 'float64']]
  for feature in numeric_features:
      plt.figure(figsize=(8, 6))
      sns.histplot(df[feature], kde=True, color='blue')
      plt.title(f'Distribution of {feature}')
      plt.xlabel(feature)
      plt.ylabel('Frequency')
      plt.show()

  # Categorical features
  categorical_features = [col for col in df.columns if df[col].dtype == 'object']
  for feature in categorical_features:
      plt.figure(figsize=(8, 6))
      sns.countplot(y=df[feature], order=df[feature].value_counts().index, palette='Set2')
      plt.title(f'Count of {feature}')
      plt.xlabel('Count')
      plt.ylabel(feature)
      plt.show()

plot_charts(df)

df.isna().sum()

df.isnull().sum()

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.model_selection import train_test_split
import pandas as pd

X = df.drop(columns=['Price'])

y = df["Price"]

numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_features = ['Manufacturer', 'Screen']

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent'))
])

cat_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', cat_transformer, cat_features)
    ])

from sklearn.preprocessing import PolynomialFeatures, StandardScaler

poly_reg = PolynomialFeatures(degree=2)

poly_reg_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', poly_reg)
])

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

X_train_poly_scaled = poly_reg_pipeline.fit_transform(X_train)
X_valid_poly_scaled = poly_reg_pipeline.transform(X_val)
X_test_poly_scaled = poly_reg_pipeline.transform(X_test)

X_train_poly_scaled

X_valid_poly_scaled

X_test_poly_scaled

"""# Ridge Regression"""

from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
import numpy as np

parameters = {'alpha': [0.01, 0.1, 1, 10, 100]}
ridge = Ridge()
grid_search = GridSearchCV(ridge, parameters, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train_poly_scaled, y_train)

best_alpha = grid_search.best_params_['alpha']

best_alpha

ridge = Ridge(alpha=best_alpha)
ridge.fit(X_train_poly_scaled, y_train)

"""Loss Function => RMSE"""

y_train_pred = ridge.predict(X_train_poly_scaled)
y_valid_pred = ridge.predict(X_valid_poly_scaled)
y_test_pred = ridge.predict(X_test_poly_scaled)

train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
valid_rmse = np.sqrt(mean_squared_error(y_val, y_valid_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

cv_rmse = np.sqrt(-cross_val_score(ridge, X_train_poly_scaled, y_train, cv=5, scoring='neg_mean_squared_error').mean())

print("Train RMSE:", train_rmse)
print("Validation RMSE:", valid_rmse)
print("Test RMSE:", test_rmse)
print("Cross-Validation RMSE:", cv_rmse)

"""Without Polynomial Features"""

X_train_encoded =  preprocessor.fit_transform(X_train)
X_val_encoded = preprocessor.transform(X_val)
X_test_encoded = preprocessor.transform(X_test)

parameters = {'alpha': [0.01, 0.1, 1, 10, 100]}
ridge = Ridge()
grid_search = GridSearchCV(ridge, parameters, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train_encoded, y_train)

best_alpha = grid_search.best_params_['alpha']

best_alpha

ridge_without_pf = Ridge(alpha=best_alpha)

ridge_without_pf.fit(X_train_encoded, y_train)

"""Loss Function => RMSE"""

y_train_pred = ridge_without_pf.predict(X_train_encoded)
y_valid_pred = ridge_without_pf.predict(X_val_encoded)
y_test_pred = ridge_without_pf.predict(X_test_encoded)

train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
valid_rmse = np.sqrt(mean_squared_error(y_val, y_valid_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

cv_rmse = np.sqrt(-cross_val_score(ridge_without_pf, X_train_encoded, y_train, cv=5, scoring='neg_mean_squared_error').mean())

print("Train RMSE:", train_rmse)
print("Validation RMSE:", valid_rmse)
print("Test RMSE:", test_rmse)
print("Cross-Validation RMSE:", cv_rmse)

"""#Lasso Regression"""

X_train_poly_scaled = poly_reg_pipeline.fit_transform(X_train)
X_valid_poly_scaled = poly_reg_pipeline.transform(X_val)
X_test_poly_scaled = poly_reg_pipeline.transform(X_test)

from sklearn.linear_model import Lasso

parameters = {'alpha': [0.01, 0.1, 1, 10, 100]}
lasso = Lasso(max_iter=10000)  # max_iter increased to avoid ConvergenceWarning
grid_search = GridSearchCV(lasso, parameters, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train_poly_scaled, y_train)

best_alpha = grid_search.best_params_['alpha']

best_alpha

lasso = Lasso(alpha=best_alpha, max_iter=10000)
lasso.fit(X_train_poly_scaled, y_train)

"""Loss Function => RMSE"""

y_train_pred = lasso.predict(X_train_poly_scaled)
y_valid_pred = lasso.predict(X_valid_poly_scaled)
y_test_pred = lasso.predict(X_test_poly_scaled)

train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
valid_rmse = np.sqrt(mean_squared_error(y_val, y_valid_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

cv_rmse = np.sqrt(-cross_val_score(lasso, X_train_poly_scaled, y_train, cv=5, scoring='neg_mean_squared_error').mean())

print("Train RMSE:", train_rmse)
print("Validation RMSE:", valid_rmse)
print("Test RMSE:", test_rmse)
print("Cross-Validation RMSE:", cv_rmse)

"""Without Polunomial Features"""

X_train_encoded =  preprocessor.fit_transform(X_train)
X_val_encoded = preprocessor.transform(X_val)
X_test_encoded = preprocessor.transform(X_test)

parameters = {'alpha': [0.01, 0.1, 1, 10, 100]}
lasso = Lasso()
grid_search = GridSearchCV(lasso, parameters, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train_encoded, y_train)

best_alpha = grid_search.best_params_['alpha']

best_alpha

lasso_without_pf = Lasso(alpha=best_alpha)

lasso_without_pf.fit(X_train_encoded, y_train)

y_train_pred = lasso_without_pf.predict(X_train_encoded)
y_valid_pred = lasso_without_pf.predict(X_val_encoded)
y_test_pred = lasso_without_pf.predict(X_test_encoded)

train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
valid_rmse = np.sqrt(mean_squared_error(y_val, y_valid_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

cv_rmse = np.sqrt(-cross_val_score(lasso_without_pf, X_train_encoded, y_train, cv=5, scoring='neg_mean_squared_error').mean())

print("Train RMSE:", train_rmse)
print("Validation RMSE:", valid_rmse)
print("Test RMSE:", test_rmse)
print("Cross-Validation RMSE:", cv_rmse)

"""# Elastic Net"""

X_train_poly_scaled = poly_reg_pipeline.fit_transform(X_train)
X_valid_poly_scaled = poly_reg_pipeline.transform(X_val)
X_test_poly_scaled = poly_reg_pipeline.transform(X_test)

from sklearn.linear_model import ElasticNet

parameters = {'alpha': [0.01, 0.1, 1, 10, 100], 'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]}
elastic_net = ElasticNet(max_iter=10000)
grid_search = GridSearchCV(elastic_net, parameters, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train_poly_scaled, y_train)

best_alpha = grid_search.best_params_['alpha']
best_l1_ratio = grid_search.best_params_['l1_ratio']

best_alpha

best_l1_ratio

elastic_net = ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio, max_iter=10000)
elastic_net.fit(X_train_poly_scaled, y_train)

"""Loss Function< => RMSE"""

y_train_pred = elastic_net.predict(X_train_poly_scaled)
y_valid_pred = elastic_net.predict(X_valid_poly_scaled)
y_test_pred = elastic_net.predict(X_test_poly_scaled)

train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
valid_rmse = np.sqrt(mean_squared_error(y_val, y_valid_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

cv_rmse = np.sqrt(-cross_val_score(elastic_net, X_train_poly_scaled, y_train, cv=5, scoring='neg_mean_squared_error').mean())

print("Train RMSE:", train_rmse)
print("Validation RMSE:", valid_rmse)
print("Test RMSE:", test_rmse)
print("Cross-Validation RMSE:", cv_rmse)

"""Without Polynomial Features"""

X_train_encoded =  preprocessor.fit_transform(X_train)
X_val_encoded = preprocessor.transform(X_val)
X_test_encoded = preprocessor.transform(X_test)

parameters = {'alpha': [0.01, 0.1, 1, 10, 100], 'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]}
elastic_net = ElasticNet(max_iter=10000)
grid_search = GridSearchCV(elastic_net, parameters, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train_encoded, y_train)

best_alpha = grid_search.best_params_['alpha']
best_l1_ratio = grid_search.best_params_['l1_ratio']

best_alpha

best_l1_ratio

elastic_net = ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio, max_iter=10000)
elastic_net.fit(X_train_encoded, y_train)

"""Loss Function => RMSE"""

y_train_pred = elastic_net.predict(X_train_encoded)
y_valid_pred = elastic_net.predict(X_val_encoded)
y_test_pred = elastic_net.predict(X_test_encoded)

train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
valid_rmse = np.sqrt(mean_squared_error(y_val, y_valid_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

cv_rmse = np.sqrt(-cross_val_score(elastic_net, X_train_encoded, y_train, cv=5, scoring='neg_mean_squared_error').mean())

print("Train RMSE:", train_rmse)
print("Validation RMSE:", valid_rmse)
print("Test RMSE:", test_rmse)
print("Cross-Validation RMSE:", cv_rmse)

"""# Early Stopping with SGD"""

X_train_poly_scaled = poly_reg_pipeline.fit_transform(X_train)
X_valid_poly_scaled = poly_reg_pipeline.transform(X_val)
X_test_poly_scaled = poly_reg_pipeline.transform(X_test)

from sklearn.linear_model import SGDRegressor

sgd = SGDRegressor(penalty='elasticnet', max_iter=1000, tol=1e-3, early_stopping=True, validation_fraction=0.2)
sgd.fit(X_train_poly_scaled, y_train)

"""Loss Function => RMSE"""

y_train_pred = sgd.predict(X_train_poly_scaled)
y_valid_pred = sgd.predict(X_valid_poly_scaled)
y_test_pred = sgd.predict(X_test_poly_scaled)

train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
valid_rmse = np.sqrt(mean_squared_error(y_val, y_valid_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

cv_rmse = np.sqrt(-cross_val_score(sgd, X_train_poly_scaled, y_train, cv=5, scoring='neg_mean_squared_error').mean())

print("Train RMSE:", train_rmse)
print("Validation RMSE:", valid_rmse)
print("Test RMSE:", test_rmse)
print("Cross-Validation RMSE:", cv_rmse)

"""Without Polynomial Features"""

X_train_encoded =  preprocessor.fit_transform(X_train)
X_val_encoded = preprocessor.transform(X_val)
X_test_encoded = preprocessor.transform(X_test)

sgd = SGDRegressor(penalty='elasticnet', max_iter=1000, tol=1e-3, early_stopping=True, validation_fraction=0.2)
sgd.fit(X_train_encoded, y_train)

"""Loss Function => RMSE"""

y_train_pred = sgd.predict(X_train_encoded)
y_valid_pred = sgd.predict(X_val_encoded)
y_test_pred = sgd.predict(X_test_encoded)

train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
valid_rmse = np.sqrt(mean_squared_error(y_val, y_valid_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

cv_rmse = np.sqrt(-cross_val_score(sgd, X_train_encoded, y_train, cv=5, scoring='neg_mean_squared_error').mean())

print("Train RMSE:", train_rmse)
print("Validation RMSE:", valid_rmse)
print("Test RMSE:", test_rmse)
print("Cross-Validation RMSE:", cv_rmse)

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression

logistic_model = LogisticRegression()

param_grid = {
    'penalty': ['l1', 'l2'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'solver': ['liblinear', 'saga']
}

X_train_encoded =  preprocessor.fit_transform(X_train)
X_val_encoded = preprocessor.transform(X_val)
X_test_encoded = preprocessor.transform(X_test)

grid_search = GridSearchCV(logistic_model, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train_encoded, y_train)

best_model = grid_search.best_estimator_

best_model

"""Loss Function => RMSE"""

y_train_pred = best_model.predict(X_train_encoded)
y_valid_pred = best_model.predict(X_val_encoded)
y_test_pred = best_model.predict(X_test_encoded)

train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
valid_rmse = np.sqrt(mean_squared_error(y_val, y_valid_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

cv_rmse = np.sqrt(-cross_val_score(best_model, X_train_encoded, y_train, cv=3, scoring='neg_mean_squared_error').mean())

print("Train RMSE:", train_rmse)
print("Validation RMSE:", valid_rmse)
print("Test RMSE:", test_rmse)
print("Cross-Validation RMSE:", cv_rmse)