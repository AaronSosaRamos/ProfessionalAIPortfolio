# -*- coding: utf-8 -*-
"""CB001 - Chapter 26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P-C4Pw67mfEzi4-LsxwbYVzP_DnBp6Xz

# Momentum Optimization
"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import SGD

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model
model = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Step 3: Define the optimizer with momentum
optimizer = SGD(learning_rate=0.01, momentum=0.9)

# Step 4: Compile the model with the specified optimizer
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Step 6: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""#Nesterov Accelerated Gradient"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import SGD

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model
model = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Step 3: Define the optimizer with Nesterov Accelerated Gradient
optimizer = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)

# Step 4: Compile the model with the specified optimizer
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Step 6: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""#AdaGrad"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adagrad

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model
model = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Step 3: Define the optimizer with AdaGrad
optimizer = Adagrad(learning_rate=0.01)

# Step 4: Compile the model with the specified optimizer
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Step 6: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""#RMSProp"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import RMSprop

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model
model = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Step 3: Define the optimizer with RMSProp
optimizer = RMSprop(learning_rate=0.001)

# Step 4: Compile the model with the specified optimizer
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Step 6: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""#Adam and Nadam Optimization"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model
model = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Step 3: Define the optimizer with Adam
optimizer = Adam(learning_rate=0.001)

# Step 4: Compile the model with the specified optimizer
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Step 6: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Nadam

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model
model = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Step 3: Define the optimizer with Nadam
optimizer = Nadam(learning_rate=0.001)

# Step 4: Compile the model with the specified optimizer
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Step 6: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""#Adamax"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adamax

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model
model = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Step 3: Define the optimizer with Adamax
optimizer = Adamax(learning_rate=0.001)

# Step 4: Compile the model with the specified optimizer
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Step 6: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""# Learning Rate Scheduling

# Power scheduling

![image.png](data:image/png;base64,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)
"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.callbacks import LearningRateScheduler

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model
model = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Step 3: Define learning rate scheduler (Power scheduling)
initial_learning_rate = 0.01
decay_rate = 0.01
power = 0.5
def lr_scheduler(epoch):
    return initial_learning_rate / (1 + decay_rate * epoch) ** power

# Step 4: Compile the model with SGD optimizer and learning rate scheduler
optimizer = SGD(learning_rate=initial_learning_rate)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Step 5: Train the model with learning rate scheduler
scheduler_callback = LearningRateScheduler(lr_scheduler)
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[scheduler_callback])

# Step 6: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""#Exponential Scheduling

![image.png](data:image/png;base64,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)
"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.callbacks import LearningRateScheduler

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model
model = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Step 3: Define learning rate scheduler (Exponential scheduling)
initial_learning_rate = 0.01
decay_rate = 0.1
def lr_scheduler(epoch):
    return initial_learning_rate * tf.math.exp(-decay_rate * epoch)

# Step 4: Compile the model with SGD optimizer and learning rate scheduler
optimizer = SGD(learning_rate=initial_learning_rate)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Step 5: Train the model with learning rate scheduler
scheduler_callback = LearningRateScheduler(lr_scheduler)
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[scheduler_callback])

# Step 6: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""#Piecewise Constant Scheduling"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model
model = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Step 3: Define piecewise constant learning rate schedule
boundaries = [5, 10]  # Epochs at which to change learning rate
values = [0.01, 0.005, 0.001]  # Learning rates at each boundary
learning_rate_fn = PiecewiseConstantDecay(boundaries, values)

# Step 4: Compile the model with SGD optimizer and piecewise constant learning rate
optimizer = SGD(learning_rate=learning_rate_fn)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(x_train, y_train, epochs=15, batch_size=32, validation_split=0.2)

# Step 6: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""#Performance Scheduling"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.callbacks import ReduceLROnPlateau

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model
model = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Step 3: Compile the model with SGD optimizer
optimizer = SGD(learning_rate=0.01)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Step 4: Define performance-based learning rate scheduler
reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_lr=0.0001)

# Step 5: Train the model with performance-based learning rate scheduler
history = model.fit(x_train, y_train, epochs=15, batch_size=32, validation_split=0.2, callbacks=[reduce_lr_callback])

# Step 6: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""# Avoiding Overfitting Through Regularization

# ℓ1 and ℓ2 Regularization
"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer, Dense
from tensorflow.keras import regularizers

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model with ℓ1 and ℓ2 regularization
model = Sequential([
    InputLayer(input_shape=(784,)),
    Dense(100, activation='elu', kernel_initializer='he_normal',
          kernel_regularizer=regularizers.l2(0.01)),
    Dense(10, activation='softmax')
])

# Step 3: Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Step 4: Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Step 5: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""#Dropout"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model with Dropout regularization
model = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dropout(0.2),  # Apply Dropout with dropout rate of 0.2 (20%)
    Dense(64, activation='relu'),
    Dropout(0.2),  # Apply Dropout with dropout rate of 0.2 (20%)
    Dense(10, activation='softmax')
])

# Step 3: Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Step 4: Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Step 5: Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

"""# Monte-Carlo (MC) Dropout

"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras import backend as K

# Step 1: Load and preprocess the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape((x_train.shape[0], 28 * 28))
x_test = x_test.reshape((x_test.shape[0], 28 * 28))
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Step 2: Build the neural network model with Dropout (MC Dropout)
model = Sequential([
    Dense(128, activation='relu', input_shape=(784,)),
    Dropout(0.2),  # Apply Dropout with dropout rate of 0.2 (20%)
    Dense(64, activation='relu'),
    Dropout(0.2),  # Apply Dropout with dropout rate of 0.2 (20%)
    Dense(10, activation='softmax')
])

# Step 3: Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Step 4: Train the model
model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Step 5: Perform Monte Carlo (MC) Dropout for uncertainty estimation
n_samples = 100  # Number of Monte Carlo samples
X_test_scaled = x_test  # Assuming x_test is already scaled if needed

# Function to perform MC Dropout and compute probabilistic predictions
def predict_with_uncertainty(model, X_test_scaled, n_samples):
    y_probas = []
    # Set learning phase to 1 (dropout on) using Keras backend
    K.set_learning_phase(1)
    for _ in range(n_samples):
        y_proba = model.predict(X_test_scaled)
        y_probas.append(y_proba)
    K.set_learning_phase(0)  # Reset learning phase to default (0)

    y_probas = np.stack(y_probas, axis=0)  # Stack predictions along new axis
    y_mean = np.mean(y_probas, axis=0)  # Compute mean prediction
    return y_mean

# Step 6: Obtain probabilistic predictions using MC Dropout
y_proba = predict_with_uncertainty(model, X_test_scaled, n_samples)

# Step 7: Evaluate the model
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {test_accuracy}')

# Optional: Compute other uncertainty metrics using y_proba (e.g., variance)
y_variance = np.var(y_proba, axis=0)