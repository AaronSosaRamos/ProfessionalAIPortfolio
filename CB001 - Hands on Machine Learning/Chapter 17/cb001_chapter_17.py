# -*- coding: utf-8 -*-
"""CB001 - Chapter 17.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mBPpHdikkAgxQer5gmlOUranZWzvc1Gr

#Stacking
"""

#Mount the google drive connection to our dataset
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd
df = pd.read_csv('/content/drive/My Drive/AI/datasets/apple_quality.csv')

df.head()

df.drop(df.tail(1).index, inplace=True)
df.drop("A_id", axis=1, inplace=True)
df["Acidity"] = df["Acidity"].astype(float)

df.info()

df.shape

df.isnull().sum()

df.isna().sum()

import matplotlib.pyplot as plt
import seaborn as sns

def plot_charts(df):
  # Numeric features
  numeric_features = [col for col in df.columns if df[col].dtype in ['int64', 'float64']]
  for feature in numeric_features:
      plt.figure(figsize=(8, 6))
      sns.histplot(df[feature], kde=True, color='blue')
      plt.title(f'Distribution of {feature}')
      plt.xlabel(feature)
      plt.ylabel('Frequency')
      plt.show()

  # Categorical features
  categorical_features = [col for col in df.columns if df[col].dtype == 'object']
  for feature in categorical_features:
      plt.figure(figsize=(8, 6))
      sns.countplot(y=df[feature], order=df[feature].value_counts().index, palette='Set2')
      plt.title(f'Count of {feature}')
      plt.xlabel('Count')
      plt.ylabel(feature)
      plt.show()

plot_charts(df)

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('scaler', StandardScaler())
])

cat_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

X = df.drop(columns=['Quality'])
y = df['Quality']

X_encoded = numeric_transformer.fit_transform(X)
X_encoded = pd.DataFrame(X_encoded, columns=X.columns)

X_encoded

y_encoded = cat_transformer.fit_transform(y.values.reshape(-1, 1))
y_encoded_df = pd.DataFrame(y_encoded.toarray(), columns=cat_transformer.named_steps['encoder'].get_feature_names_out())
y_encoded_df.drop("x0_bad", axis=1, inplace=True)
print(y_encoded_df.head())

"""* Train => 80%
* Holdout => 20%
"""

X_train, X_holdout, y_train, y_holdout = train_test_split(X_encoded, y_encoded_df, test_size=0.2, random_state=42)

"""Train Base Models

"""

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
import numpy as np

base_model1 = LogisticRegression()
base_model2 = GradientBoostingClassifier()
base_model3 = SVC(kernel='linear')

base_model1.fit(X_train, y_train)
base_model2.fit(X_train, y_train)
base_model3.fit(X_train, y_train)

"""Generate Base Model Predictions

"""

preds_base_model1 = base_model1.predict(X_holdout)
preds_base_model2 = base_model2.predict(X_holdout)
preds_base_model3 = base_model3.predict(X_holdout)

"""Create Meta-Model Input

"""

meta_model_input = np.column_stack((preds_base_model1, preds_base_model2, preds_base_model3))

"""Train Meta-Model"""

meta_model = RandomForestClassifier()
meta_model.fit(meta_model_input, y_holdout)

from sklearn.metrics import accuracy_score

meta_preds_test = meta_model.predict(meta_model_input)

accuracy = accuracy_score(y_holdout, meta_preds_test)
print("Accuracy:", accuracy)

"""#Dimensionality Reduction

#2 main approaches:
* Projection
* Manifold Learning

#PCA
"""

from sklearn.decomposition import PCA
pca = PCA(n_components = 2)
X2D = pca.fit_transform(X_encoded)

"""Variance Ratio:"""

pca.explained_variance_ratio_

pca = PCA(n_components = 4)
X2D = pca.fit_transform(X_encoded)

pca.explained_variance_ratio_

"""#Choose the right number of components"""

pca = PCA()
pca.fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.95) + 1
pca = PCA(n_components=d)
X2D = pca.fit_transform(X_encoded)

d

pca.explained_variance_ratio_

pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X_train)

pca.explained_variance_ratio_