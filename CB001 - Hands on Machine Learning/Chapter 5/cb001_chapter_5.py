# -*- coding: utf-8 -*-
"""CB001 - Chapter 5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15TrXXUUpNP6kNqRAF5TmGB6M76GZUj0Q

#Decision Trees and Random Forests

Decision Tree:
*   Tree-Like Structure
*   Each internal node is a test
*   Each branch is the outcome
*   Each leaf-node is a variable

Random Forests:
*   Ensemble Learning Method
*   Many Decision Trees
*   Mode => Classification Output
*   Mean => Regression Output

Connect to Google Drive
"""

#Mount the google drive connection to our dataset
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""# Classification Task"""

import pandas as pd
df_classification_train = pd.read_csv('/content/drive/My Drive/AI/datasets/mobile_price_classification_train.csv')

df_classification_train.head()

df_classification_train.info()

df_classification_train.shape

"""Fill Null and NA data"""

df_classification_train.isnull().sum()

"""Visualize the data"""

import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(nrows=6, ncols=4, figsize=(20, 30))

sns.histplot(df_classification_train['battery_power'], ax=axes[0, 0])
axes[0, 0].set_title('Battery Power Distribution')

binary_vars = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']
for i, var in enumerate(binary_vars):
    sns.countplot(x=var, data=df_classification_train, ax=axes[i//4, i%4])
    axes[i//4, i%4].set_title(f'{var.capitalize()} Count')

continuous_vars = ['clock_speed', 'm_dep', 'mobile_wt', 'n_cores', 'px_height', 'px_width',
                   'ram', 'sc_h', 'sc_w', 'talk_time']
for i, var in enumerate(continuous_vars):
    sns.histplot(df_classification_train[var], ax=axes[(i+len(binary_vars))//4, (i+len(binary_vars))%4])
    axes[(i+len(binary_vars))//4, (i+len(binary_vars))%4].set_title(f'{var.capitalize()} Distribution')

for i, var in enumerate(['fc', 'int_memory', 'pc']):
    sns.boxplot(x=var, data=df_classification_train, ax=axes[(i+len(binary_vars)+len(continuous_vars))//4,
                                                              (i+len(binary_vars)+len(continuous_vars))%4])
    axes[(i+len(binary_vars)+len(continuous_vars))//4,
         (i+len(binary_vars)+len(continuous_vars))%4].set_title(f'{var.upper()} Box Plot')

sns.countplot(x='price_range', data=df_classification_train, ax=axes[-1, -1])
axes[-1, -1].set_title('Price Range Count')

plt.tight_layout()
plt.show()

df_classification_test = pd.read_csv('/content/drive/My Drive/AI/datasets/mobile_price_classification_test.csv')

df_classification_test.head()

df_classification_test.info()

df_classification_test.drop("id", axis=1, inplace=True)

df_classification_test.info()

df_classification_test.shape

"""Divide the data:


*   Train DF => Train (60%), Test (20%) and Validation (20%)
*   Test DF => Prediction1 (50%) and Prediction2 (50%)


"""

X = df_classification_train.drop("price_range", axis=1)
y = df_classification_train["price_range"]

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

"""# Decision Tree Classifier"""

dt_clf = DecisionTreeClassifier()
dt_clf.fit(X_train, y_train)

"""Loss Function => Accuracy"""

cv_scores = cross_val_score(dt_clf, X_train, y_train, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean Cross-Validation Accuracy:", cv_scores.mean())

y_pred_val = dt_clf.predict(X_val)
val_accuracy = accuracy_score(y_val, y_pred_val)
print("Validation Accuracy:", val_accuracy)

y_pred_test = dt_clf.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print("Test Accuracy:", test_accuracy)

"""# Random Forest Classifier"""

rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train, y_train)

"""Loss Function => Accuracy"""

cv_scores = cross_val_score(rf_clf, X_train, y_train, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean Cross-Validation Accuracy:", cv_scores.mean())

y_pred_val = rf_clf.predict(X_val)
val_accuracy = accuracy_score(y_val, y_pred_val)
print("Validation Accuracy:", val_accuracy)

y_pred_test = rf_clf.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print("Test Accuracy:", test_accuracy)

"""Predictions"""

dtc_predictions = dt_clf.predict(df_classification_test.iloc[0:df_classification_test.shape[0]//2])
rfc_predictions = rf_clf.predict(df_classification_test.iloc[df_classification_test.shape[0]//2:df_classification_test.shape[0]])

print(dtc_predictions)

print(rfc_predictions)

"""# Regression Task"""

import pandas as pd
df_regression = pd.read_csv('/content/drive/My Drive/AI/datasets/Steel_industry.csv')

df_regression.head()

df_regression.info()

df_regression["Usage_kWh"].value_counts()

df_regression.shape

df_regression.drop("Date_Time", axis=1, inplace=True)

"""Analyse categorical variables"""

df_regression["WeekStatus"].value_counts()

df_regression["Day_Of_Week"].value_counts()

df_regression["Load_Type"].value_counts()

df_regression.isnull().sum()

"""Encode categorical variables"""

import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

onehot_cols = ['WeekStatus']
ordinal_cols = ['Day_Of_Week', 'Load_Type']

onehot_pipeline = Pipeline([
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

ordinal_pipeline = Pipeline([
    ('ordinal', OrdinalEncoder())
])

preprocessor = ColumnTransformer(transformers=[
    ('onehot', onehot_pipeline, onehot_cols),
    ('ordinal', ordinal_pipeline, ordinal_cols)
], remainder='passthrough')

processed_data = preprocessor.fit_transform(df_regression)

encoded_onehot_cols = preprocessor.named_transformers_['onehot'].named_steps['onehot'] \
                      .get_feature_names_out(input_features=onehot_cols)

encoded_cols = list(encoded_onehot_cols) + ordinal_cols + list(df_regression.columns.drop(onehot_cols + ordinal_cols))

processed_df = pd.DataFrame(processed_data, columns=encoded_cols)
print(processed_df)

processed_df.info()

day_week = df_regression["Day_Of_Week"].unique()

comparison_df = pd.DataFrame({"Old_Day_Of_Week": day_week,
                              "Encoded_Day_Of_Week": processed_df["Day_Of_Week"].unique()})

comparison_df = comparison_df.sort_values(by="Old_Day_Of_Week").reset_index(drop=True)

print(comparison_df)

load_type = df_regression["Load_Type"].unique()

comparison_df = pd.DataFrame({"Old_Load_Type": load_type,
                              "Encoded_Load_Type": processed_df["Load_Type"].unique()})

comparison_df = comparison_df.sort_values(by="Old_Load_Type").reset_index(drop=True)

print(comparison_df)

"""Visualize the data"""

import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))

sns.histplot(processed_df['WeekStatus_Weekday'], ax=axes[0, 0])
axes[0, 0].set_title('Weekday Status Distribution')

sns.histplot(processed_df['WeekStatus_Weekend'], ax=axes[0, 1])
axes[0, 1].set_title('Weekend Status Distribution')

sns.histplot(processed_df['Day_Of_Week'], ax=axes[0, 2])
axes[0, 2].set_title('Day of Week Distribution')

sns.histplot(processed_df['Load_Type'], ax=axes[0, 3])
axes[0, 3].set_title('Load Type Distribution')

sns.histplot(processed_df['Usage_kWh'], ax=axes[1, 0])
axes[1, 0].set_title('Usage_kWh Distribution')

sns.histplot(processed_df['Lagging_Current_Reactive.Power_kVarh'], ax=axes[1, 1])
axes[1, 1].set_title('Lagging Current Reactive Power Distribution')

sns.histplot(processed_df['Leading_Current_Reactive_Power_kVarh'], ax=axes[1, 2])
axes[1, 2].set_title('Leading Current Reactive Power Distribution')

sns.histplot(processed_df['CO2(tCO2)'], ax=axes[1, 3])
axes[1, 3].set_title('CO2 Distribution')

sns.histplot(processed_df['Lagging_Current_Power_Factor'], ax=axes[2, 0])
axes[2, 0].set_title('Lagging Current Power Factor Distribution')

sns.histplot(processed_df['Leading_Current_Power_Factor'], ax=axes[2, 1])
axes[2, 1].set_title('Leading Current Power Factor Distribution')

sns.histplot(processed_df['NSM'], ax=axes[2, 2])
axes[2, 2].set_title('NSM Distribution')

fig.delaxes(axes[2, 3])

plt.tight_layout()
plt.show()

X = processed_df.drop("Usage_kWh", axis=1)
y = processed_df["Usage_kWh"]

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

"""# Decision Tree Regressor"""

dt_reg = DecisionTreeRegressor()
dt_reg.fit(X_train, y_train)

cv_scores = cross_val_score(dt_reg, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
print("Cross-Validation Scores:", cv_scores)
print("Mean Cross-Validation MSE:", -cv_scores.mean())

y_pred_val = dt_reg.predict(X_val)
val_mse = mean_squared_error(y_val, y_pred_val)
print("Validation Mean Squared Error:", val_mse)

y_pred_test = dt_reg.predict(X_test)
test_mse = mean_squared_error(y_test, y_pred_test)
print("Test Mean Squared Error:", test_mse)

"""# Random Forest Regressor"""

rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)

cv_scores = cross_val_score(rf_reg, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
print("Cross-Validation Scores:", cv_scores)
print("Mean Cross-Validation MSE:", -cv_scores.mean())

y_pred_val = rf_reg.predict(X_val)
val_mse = mean_squared_error(y_val, y_pred_val)
print("Validation Mean Squared Error:", val_mse)

y_pred_test = rf_reg.predict(X_test)
test_mse = mean_squared_error(y_test, y_pred_test)
print("Test Mean Squared Error:", test_mse)

"""Predictions"""

df_for_test = {
    'WeekStatus_Weekday': 1.0,
    'WeekStatus_Weekend': 0.0,
    'Day_Of_Week': 3.0,
    'Load_Type': 1.0,
    'Lagging_Current_Reactive.Power_kVarh': 5.26,
    'Leading_Current_Reactive_Power_kVarh': 0.09,
    'CO2(tCO2)': 0.1,
    'Lagging_Current_Power_Factor': 77.79,
    'Leading_Current_Power_Factor': 99.98,
    'NSM': 82760.35
}

df_for_test = pd.DataFrame([df_for_test])

dtr_prediction = dt_reg.predict(df_for_test)
print(dtr_prediction)

rfr_prediction = rf_reg.predict(df_for_test)
print(rfr_prediction)