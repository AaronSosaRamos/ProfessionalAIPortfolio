# -*- coding: utf-8 -*-
"""CB001 - Chapter 20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fAsjWwkb-jcFjuxrT45QcfMKGqeHemBS

#DBSCAN

Method 1:
"""

from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=1000, noise=0.05)
dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(X)

dbscan.labels_

len(dbscan.core_sample_indices_)

dbscan.core_sample_indices_

dbscan.components_

clusters = dbscan.fit_predict(X)

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50, alpha=0.5)
plt.title('DBSCAN Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Cluster')
plt.show()

"""Method 2:"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN

# Generating sample dataset
X, _ = make_moons(n_samples=500, noise=0.1, random_state=42)

# Applying DBSCAN clustering
dbscan = DBSCAN(eps=0.2, min_samples=5)
clusters = dbscan.fit_predict(X)

# Visualizing the dataset with different colors for clusters
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50, alpha=0.5)
plt.title('DBSCAN Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Cluster')
plt.show()

"""#Use KNN for predictions:"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=50)
knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])

X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])

knn.predict(X_new)

knn.predict_proba(X_new)

"""Use kneighbors for setting the maxium distances (Anomaly classification)"""

y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]
y_pred[y_dist > 0.2] = -1
y_pred.ravel()

y_pred_knn = knn.predict(X_new)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN

# Generating sample dataset
X, y = make_moons(n_samples=500, noise=0.1, random_state=42)

# K-nearest neighbors classification
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X, y)

# Generating new data points for prediction
X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])

# Predicting labels using KNN
y_pred_knn = knn.predict(X_new)
y_proba_knn = knn.predict_proba(X_new)

# DBSCAN clustering
dbscan = DBSCAN(eps=0.2, min_samples=5)
dbscan.fit(X)

# Finding nearest neighbors and predicting labels using KNN
y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
y_pred_knn = y[y_pred_idx.ravel()]

# Finding nearest neighbors and predicting labels using DBSCAN
y_pred_dbscan = dbscan.labels_[dbscan.core_sample_indices_]
y_pred_dbscan = np.array([y_pred_dbscan[point] if dist <= 0.2 else -1 for dist, point in zip(y_dist.ravel(), y_pred_idx.ravel())])

# Plotting the cluster classification diagram
plt.figure(figsize=(12, 6))

# KNN plot
plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=50, alpha=0.5)
plt.scatter(X_new[:, 0], X_new[:, 1], c=y_pred_knn, cmap='viridis', marker='x', s=100, linewidths=2)
plt.title('KNN Classification')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

# DBSCAN plot
plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=dbscan.labels_, cmap='viridis', s=50, alpha=0.5)
plt.scatter(X_new[:, 0], X_new[:, 1], c=y_pred_dbscan, cmap='viridis', marker='x', s=100, linewidths=2)
plt.title('DBSCAN Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

plt.tight_layout()
plt.show()

"""#Gaussian Mixture"""

from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# Generating synthetic data
X, y = make_blobs(n_samples=1000, centers=3, cluster_std=1.0, random_state=42)

# Plotting the generated data
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=20)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Generated Dataset')
plt.show()

from sklearn.mixture import GaussianMixture
import numpy as np

# Creating and fitting the Gaussian Mixture Model
gmm = GaussianMixture(n_components=3, random_state=42)
gmm.fit(X)

# Predicting the clusters
y_pred = gmm.predict(X)

# Plotting the clusters
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=20)
plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], marker='x', color='red', s=100, label='Cluster Centers')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Gaussian Mixture Model Clusters')
plt.legend()
plt.show()

gmm.weights_

gmm.means_

gmm.covariances_

gmm.converged_

gmm.n_iter_

gmm.predict(X)

gmm.predict_proba(X)

"""Generative model:"""

X_new, y_new = gmm.sample(6)

X_new

gmm.score_samples(X)

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs

# Generating synthetic data
X, y = make_blobs(n_samples=1000, centers=3, cluster_std=1.0, random_state=42)

# Creating and fitting the Gaussian Mixture Model
gmm = GaussianMixture(n_components=3, random_state=42)
gmm.fit(X)

# Create meshgrid for plotting decision boundaries
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                     np.arange(y_min, y_max, 0.01))
Z = gmm.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plotting the clusters with decision boundaries and density contours
plt.figure(figsize=(10, 6))

plt.contour(xx, yy, Z, levels=np.arange(3)-0.5, colors='black', linewidths=1)

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=20, alpha=0.6)
plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], marker='x', color='red', s=100, label='Cluster Means')

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Gaussian Mixture Model')
plt.legend()
plt.colorbar(label='Cluster')

plt.show()

"""#Anomaly Detection using Gaussian Mixtures"""

densities = gmm.score_samples(X)
density_threshold = np.percentile(densities, 4)
anomalies = X[densities < density_threshold]

anomalies

import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs

# Generating synthetic data
X, y = make_blobs(n_samples=1000, centers=3, cluster_std=1.0, random_state=42)

# Creating and fitting the Gaussian Mixture Model
gmm = GaussianMixture(n_components=3, random_state=42)
gmm.fit(X)

# Computing densities
densities = gmm.score_samples(X)
density_threshold = np.percentile(densities, 4)
anomalies = X[densities < density_threshold]

# Plotting the densities, density threshold, and anomalies
plt.figure(figsize=(10, 6))

plt.scatter(X[:, 0], X[:, 1], c=densities, cmap='viridis', s=20, alpha=0.6)
plt.colorbar(label='Density')

plt.scatter(anomalies[:, 0], anomalies[:, 1], color='red', marker='x', s=100, label='Anomalies')

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Gaussian Mixture Model: Densities and Anomalies')
plt.legend()

plt.show()

"""#Selecting the Number of Clusters

find the model that minimizes a theoretical information criterion
* BIC (Bayesian information criterion (BIC))
* AIC (Akaike information criterion (AIC))
"""

gmm.bic(X)

gmm.aic(X)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs

# Generating synthetic data
X, y = make_blobs(n_samples=1000, centers=3, cluster_std=1.0, random_state=42)

# Range of cluster numbers
n_components_range = range(1, 11)

# Fit GMMs and compute BIC and AIC
bic = []
aic = []
for n_components in n_components_range:
    gmm = GaussianMixture(n_components=n_components, random_state=42)
    gmm.fit(X)
    bic.append(gmm.bic(X))
    aic.append(gmm.aic(X))

# Plotting BIC and AIC
plt.figure(figsize=(10, 6))
plt.plot(n_components_range, bic, label='BIC', marker='o')
plt.plot(n_components_range, aic, label='AIC', marker='x')
plt.xlabel('Number of Clusters')
plt.ylabel('Information Criterion Value')
plt.title('BIC and AIC for Gaussian Mixture Models')
plt.xticks(n_components_range)
plt.legend()
plt.grid(True)
plt.show()

"""#Other Clustering Algorithms

#Agglomerative clustering
"""

from sklearn.datasets import make_blobs
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

# Generating synthetic data
X, y = make_blobs(n_samples=1000, centers=5, cluster_std=1.0, random_state=42)

# Applying Agglomerative clustering
agg_cluster = AgglomerativeClustering(n_clusters=5)
agg_cluster.fit(X)

# Plotting the clusters
plt.scatter(X[:, 0], X[:, 1], c=agg_cluster.labels_, cmap='viridis', s=20)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Agglomerative Clustering')
plt.show()

"""#Birch"""

from sklearn.datasets import make_blobs
from sklearn.cluster import Birch
import matplotlib.pyplot as plt

# Generating synthetic data
X, y = make_blobs(n_samples=1000, centers=5, cluster_std=1.0, random_state=42)

# Applying Birch clustering
birch_cluster = Birch(n_clusters=5)
birch_cluster.fit(X)

# Plotting the clusters
plt.scatter(X[:, 0], X[:, 1], c=birch_cluster.labels_, cmap='viridis', s=20)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Birch Clustering')
plt.show()

"""#Mean-Shift:"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs, make_gaussian_quantiles
from sklearn.cluster import MeanShift

# Generating synthetic data with varying densities
X1, _ = make_blobs(n_samples=500, centers=[[0, 0], [1, 1]], cluster_std=0.3, random_state=42)
X2, _ = make_gaussian_quantiles(mean=(2, 2), cov=1.5, n_samples=500, n_features=2, n_classes=1, random_state=42)
X = np.vstack((X1, X2))

# Applying Mean Shift clustering
mean_shift_cluster = MeanShift()
mean_shift_cluster.fit(X)

# Plotting the clusters
plt.figure(figsize=(8, 6))
labels = mean_shift_cluster.labels_
cluster_centers = mean_shift_cluster.cluster_centers_

unique_labels = np.unique(labels)
colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))

for i, color in zip(unique_labels, colors):
    class_members = labels == i
    cluster_center = cluster_centers[i]
    plt.scatter(X[class_members, 0], X[class_members, 1], c=[color], s=20)
    plt.scatter(cluster_center[0], cluster_center[1], c='black', marker='x', s=200, linewidths=3)

plt.title('Mean Shift Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

"""#Affinity propagation"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import AffinityPropagation
from itertools import cycle

# Generating synthetic data (moons)
X, _ = make_moons(n_samples=300, noise=0.1, random_state=42)

# Applying Affinity Propagation clustering
affinity_propagation_cluster = AffinityPropagation()
affinity_propagation_cluster.fit(X)

# Plotting the clusters
plt.figure(figsize=(8, 6))
colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')
for k, col in zip(range(len(np.unique(affinity_propagation_cluster.labels_))), colors):
    class_members = affinity_propagation_cluster.labels_ == k
    cluster_center = X[affinity_propagation_cluster.cluster_centers_indices_[k]]
    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')
    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
             markeredgecolor='k', markersize=14)
    for x in X[class_members]:
        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)
plt.title('Affinity Propagation Clustering')
plt.show()

"""#Spectral clustering"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import SpectralClustering

# Generating synthetic data (moons)
X, _ = make_moons(n_samples=300, noise=0.1, random_state=42)

# Applying Spectral Clustering
spectral_cluster = SpectralClustering(n_clusters=2, affinity='nearest_neighbors')
spectral_cluster.fit(X)

# Plotting the clusters
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=spectral_cluster.labels_, cmap='viridis', s=20)
plt.title('Spectral Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()