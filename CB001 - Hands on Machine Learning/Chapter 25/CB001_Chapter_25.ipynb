{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Normalization"
      ],
      "metadata": {
        "id": "fAzFrXWREUeJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQvWXdHoB1OB",
        "outputId": "cf2df576-1a81-4b09-f9e3-244194844f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               100480    \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 128)               512       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 64)                256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 110154 (430.29 KB)\n",
            "Trainable params: 109770 (428.79 KB)\n",
            "Non-trainable params: 384 (1.50 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 13s 6ms/step - loss: 0.2463 - accuracy: 0.9276 - val_loss: 0.1323 - val_accuracy: 0.9604\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1244 - accuracy: 0.9621 - val_loss: 0.0844 - val_accuracy: 0.9746\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0947 - accuracy: 0.9712 - val_loss: 0.0980 - val_accuracy: 0.9698\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0777 - accuracy: 0.9763 - val_loss: 0.0864 - val_accuracy: 0.9737\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0660 - accuracy: 0.9789 - val_loss: 0.0815 - val_accuracy: 0.9762\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0596 - accuracy: 0.9809 - val_loss: 0.0742 - val_accuracy: 0.9778\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0515 - accuracy: 0.9832 - val_loss: 0.0711 - val_accuracy: 0.9786\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0452 - accuracy: 0.9850 - val_loss: 0.0751 - val_accuracy: 0.9758\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0417 - accuracy: 0.9861 - val_loss: 0.0764 - val_accuracy: 0.9773\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0378 - accuracy: 0.9871 - val_loss: 0.0792 - val_accuracy: 0.9785\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0792 - accuracy: 0.9785\n",
            "Validation Accuracy: 97.85%\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
        "\n",
        "# Flatten the input data\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_val = x_val.reshape(x_val.shape[0], -1)\n",
        "\n",
        "# Define the model architecture with BatchNormalization\n",
        "input_layer = Input(shape=(784,))\n",
        "x = Dense(128, activation='relu')(input_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "output_layer = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_val, y_val)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ELU Activation"
      ],
      "metadata": {
        "id": "7kM37dSSE6Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
        "\n",
        "# Flatten the input data\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_val = x_val.reshape(x_val.shape[0], -1)\n",
        "\n",
        "# Define the model architecture with BatchNormalization and ELU activations\n",
        "input_layer = Input(shape=(784,))\n",
        "x = Dense(128)(input_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('elu')(x)\n",
        "x = Dense(64)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('elu')(x)\n",
        "output_layer = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_val, y_val)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTBWPtqXE-wD",
        "outputId": "73182226-6507-456d-d1a5-479e90a35af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 128)               512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation (Activation)     (None, 128)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 64)                256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 64)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 110154 (430.29 KB)\n",
            "Trainable params: 109770 (428.79 KB)\n",
            "Non-trainable params: 384 (1.50 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2770 - accuracy: 0.9161 - val_loss: 0.1541 - val_accuracy: 0.9537\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1430 - accuracy: 0.9563 - val_loss: 0.1164 - val_accuracy: 0.9635\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1074 - accuracy: 0.9661 - val_loss: 0.0962 - val_accuracy: 0.9702\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0888 - accuracy: 0.9718 - val_loss: 0.0838 - val_accuracy: 0.9754\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0743 - accuracy: 0.9757 - val_loss: 0.0768 - val_accuracy: 0.9769\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0667 - accuracy: 0.9783 - val_loss: 0.0782 - val_accuracy: 0.9770\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0579 - accuracy: 0.9805 - val_loss: 0.0772 - val_accuracy: 0.9762\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0500 - accuracy: 0.9840 - val_loss: 0.0725 - val_accuracy: 0.9780\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0478 - accuracy: 0.9844 - val_loss: 0.0997 - val_accuracy: 0.9714\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0411 - accuracy: 0.9857 - val_loss: 0.0704 - val_accuracy: 0.9799\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.9799\n",
            "Validation Accuracy: 97.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fast RELU"
      ],
      "metadata": {
        "id": "MGYSw9ZXFonE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
        "\n",
        "# Flatten the input data\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_val = x_val.reshape(x_val.shape[0], -1)\n",
        "\n",
        "# Define the FastReLU activation function\n",
        "def fast_relu(x):\n",
        "    return tf.maximum(0.01 * x, x)\n",
        "\n",
        "# Define the model architecture with BatchNormalization and FastReLU activations\n",
        "input_layer = Input(shape=(784,))\n",
        "x = Dense(128)(input_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(fast_relu)(x)\n",
        "x = Dense(64)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(fast_relu)(x)\n",
        "output_layer = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_val, y_val)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evzg_f1HFpZL",
        "outputId": "d9dd4622-ba69-4e33-ebb1-140b3e850691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 128)               512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 128)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 64)                256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 64)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 110154 (430.29 KB)\n",
            "Trainable params: 109770 (428.79 KB)\n",
            "Non-trainable params: 384 (1.50 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2465 - accuracy: 0.9293 - val_loss: 0.1057 - val_accuracy: 0.9677\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1142 - accuracy: 0.9642 - val_loss: 0.0798 - val_accuracy: 0.9750\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0835 - accuracy: 0.9729 - val_loss: 0.0883 - val_accuracy: 0.9730\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0700 - accuracy: 0.9777 - val_loss: 0.0715 - val_accuracy: 0.9771\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0588 - accuracy: 0.9815 - val_loss: 0.0657 - val_accuracy: 0.9799\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0507 - accuracy: 0.9832 - val_loss: 0.0718 - val_accuracy: 0.9782\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0448 - accuracy: 0.9855 - val_loss: 0.0706 - val_accuracy: 0.9793\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0427 - accuracy: 0.9858 - val_loss: 0.0735 - val_accuracy: 0.9784\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0351 - accuracy: 0.9881 - val_loss: 0.0646 - val_accuracy: 0.9808\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0345 - accuracy: 0.9883 - val_loss: 0.0700 - val_accuracy: 0.9796\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0700 - accuracy: 0.9796\n",
            "Validation Accuracy: 97.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Clipping\n"
      ],
      "metadata": {
        "id": "qYyTbEROF_L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
        "\n",
        "# Flatten the input data\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_val = x_val.reshape(x_val.shape[0], -1)\n",
        "\n",
        "# Define the FastReLU activation function\n",
        "def fast_relu(x):\n",
        "    return tf.maximum(0.01 * x, x)\n",
        "\n",
        "# Define the model architecture with BatchNormalization and FastReLU activations\n",
        "input_layer = Input(shape=(784,))\n",
        "x = Dense(128)(input_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(fast_relu)(x)\n",
        "x = Dense(64)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(fast_relu)(x)\n",
        "output_layer = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Define optimizer with gradient clipping\n",
        "optimizer = tf.keras.optimizers.Adam(clipvalue=0.5)  # Set clipvalue as desired threshold\n",
        "\n",
        "# Compile the model with custom optimizer\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_val, y_val)\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIAanF9zF_ro",
        "outputId": "368a0510-226b-4e58-9e55-61a880c5381c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 128)               512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 128)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 64)                256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 110154 (430.29 KB)\n",
            "Trainable params: 109770 (428.79 KB)\n",
            "Non-trainable params: 384 (1.50 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.2510 - accuracy: 0.9277 - val_loss: 0.1051 - val_accuracy: 0.9666\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1144 - accuracy: 0.9647 - val_loss: 0.0878 - val_accuracy: 0.9726\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0878 - accuracy: 0.9720 - val_loss: 0.0713 - val_accuracy: 0.9771\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0707 - accuracy: 0.9783 - val_loss: 0.0730 - val_accuracy: 0.9771\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0581 - accuracy: 0.9813 - val_loss: 0.0777 - val_accuracy: 0.9754\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0497 - accuracy: 0.9839 - val_loss: 0.0685 - val_accuracy: 0.9781\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0461 - accuracy: 0.9855 - val_loss: 0.0729 - val_accuracy: 0.9797\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0416 - accuracy: 0.9863 - val_loss: 0.0690 - val_accuracy: 0.9800\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0383 - accuracy: 0.9869 - val_loss: 0.0720 - val_accuracy: 0.9786\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0338 - accuracy: 0.9888 - val_loss: 0.0738 - val_accuracy: 0.9795\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0738 - accuracy: 0.9795\n",
            "Validation Accuracy: 97.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reusing Pretrained Layers with Transfer Learning"
      ],
      "metadata": {
        "id": "0B4c1hzyGRA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "\n",
        "# Load MNIST dataset for pre-training\n",
        "(x_train_mnist, y_train_mnist), _ = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train_mnist = x_train_mnist / 255.0\n",
        "\n",
        "# Flatten the input data\n",
        "x_train_mnist = x_train_mnist.reshape(x_train_mnist.shape[0], -1)\n",
        "\n",
        "# Define the pre-trained model (using MNIST data)\n",
        "input_layer_mnist = Input(shape=(784,))\n",
        "x = Dense(128)(input_layer_mnist)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dense(64)(x)\n",
        "x = BatchNormalization()(x)\n",
        "output_layer_mnist = Activation('softmax')(x)\n",
        "\n",
        "pretrained_model_mnist = Model(inputs=input_layer_mnist, outputs=output_layer_mnist)\n",
        "\n",
        "# Compile the pre-trained model (for demonstration purposes)\n",
        "pretrained_model_mnist.compile(optimizer='adam',\n",
        "                               loss='sparse_categorical_crossentropy',\n",
        "                               metrics=['accuracy'])\n",
        "\n",
        "# Train the pre-trained model (using MNIST data)\n",
        "pretrained_model_mnist.fit(x_train_mnist, y_train_mnist, epochs=5, batch_size=32)\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(x_train_fashion, y_train_fashion), _ = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train_fashion = x_train_fashion / 255.0\n",
        "\n",
        "# Flatten the input data\n",
        "x_train_fashion = x_train_fashion.reshape(x_train_fashion.shape[0], -1)\n",
        "\n",
        "# Freeze the lower layers of the pre-trained model\n",
        "for layer in pretrained_model_mnist.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Replace the top layers for Fashion MNIST\n",
        "output_layer_fashion = Dense(10, activation='softmax')(pretrained_model_mnist.layers[-3].output)\n",
        "transfer_model = Model(inputs=pretrained_model_mnist.input, outputs=output_layer_fashion)\n",
        "\n",
        "# Compile the transfer model\n",
        "transfer_model.compile(optimizer='adam',\n",
        "                       loss='sparse_categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "transfer_model.summary()\n",
        "\n",
        "# Train the transfer model (using Fashion MNIST data)\n",
        "transfer_model.fit(x_train_fashion, y_train_fashion, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMcNXTmyGU2z",
        "outputId": "c035bf5c-4c0b-421a-9626-64369b174ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.8311 - accuracy: 0.8963\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2505 - accuracy: 0.9475\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1648 - accuracy: 0.9609\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1268 - accuracy: 0.9675\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1029 - accuracy: 0.9729\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               100480    \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 128)               512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 128)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109898 (429.29 KB)\n",
            "Trainable params: 650 (2.54 KB)\n",
            "Non-trainable params: 109248 (426.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5289 - accuracy: 0.5161\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0918 - accuracy: 0.6379\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 0.9952 - accuracy: 0.6507\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9560 - accuracy: 0.6571\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9353 - accuracy: 0.6607\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 0.9216 - accuracy: 0.6636\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 0.9119 - accuracy: 0.6653\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9060 - accuracy: 0.6657\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9013 - accuracy: 0.6682\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 0.8968 - accuracy: 0.6696\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dc348edc880>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unsupervised Pretraining"
      ],
      "metadata": {
        "id": "SLVd1s4GJgJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "\n",
        "# Load MNIST dataset for unsupervised pretraining\n",
        "(x_train_mnist, _), _ = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train_mnist = x_train_mnist / 255.0\n",
        "\n",
        "# Flatten the input data\n",
        "x_train_mnist = x_train_mnist.reshape(x_train_mnist.shape[0], -1)\n",
        "\n",
        "# Define and train an autoencoder model on MNIST\n",
        "input_layer = Input(shape=(784,))\n",
        "encoded = Dense(128, activation='relu')(input_layer)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dense(784, activation='sigmoid')(decoded)\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(x_train_mnist, x_train_mnist, epochs=10, batch_size=32)\n",
        "\n",
        "# Extract the encoder part from the trained autoencoder\n",
        "encoder_model = Model(inputs=input_layer, outputs=encoded)\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(x_train_fashion, y_train_fashion), _ = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train_fashion = x_train_fashion / 255.0\n",
        "\n",
        "# Flatten the input data\n",
        "x_train_fashion = x_train_fashion.reshape(x_train_fashion.shape[0], -1)\n",
        "\n",
        "# Build a neural network for Fashion MNIST using the pretrained encoder\n",
        "input_layer_fashion = Input(shape=(784,))\n",
        "encoded_features = encoder_model(input_layer_fashion)\n",
        "x = Dense(64, activation='relu')(encoded_features)\n",
        "x = BatchNormalization()(x)\n",
        "output_layer_fashion = Dense(10, activation='softmax')(x)\n",
        "\n",
        "classifier_model = Model(inputs=input_layer_fashion, outputs=output_layer_fashion)\n",
        "\n",
        "# Compile the classifier model\n",
        "classifier_model.compile(optimizer='adam',\n",
        "                         loss='sparse_categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "classifier_model.summary()\n",
        "\n",
        "# Train the classifier model (using Fashion MNIST data)\n",
        "classifier_model.fit(x_train_fashion, y_train_fashion, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dyerq-BwJgoh",
        "outputId": "9864438f-ea29-4adb-d554-0fd8f833f57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.1306\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0910\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0843\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0810\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0789\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0774\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0763\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0756\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0750\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0746\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " model_7 (Functional)        (None, 64)                108736    \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 64)                256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113802 (444.54 KB)\n",
            "Trainable params: 113674 (444.04 KB)\n",
            "Non-trainable params: 128 (512.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.5175 - accuracy: 0.8196\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3827 - accuracy: 0.8612\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3502 - accuracy: 0.8721\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.3247 - accuracy: 0.8821\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3095 - accuracy: 0.8864\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2968 - accuracy: 0.8913\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2823 - accuracy: 0.8950\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2706 - accuracy: 0.8990\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2605 - accuracy: 0.9032\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2516 - accuracy: 0.9071\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dc36698a230>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretraining on an Auxiliary Task"
      ],
      "metadata": {
        "id": "Oo8bg8CqJ2pJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "import numpy as np\n",
        "\n",
        "# Load MNIST dataset for unsupervised pretraining\n",
        "(x_train_mnist, _), _ = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train_mnist = x_train_mnist / 255.0\n",
        "\n",
        "# Add random noise to the MNIST images for denoising task\n",
        "noise_factor = 0.2\n",
        "x_train_noisy = x_train_mnist + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train_mnist.shape)\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)  # Clip values to ensure they are within [0, 1]\n",
        "\n",
        "# Reshape input data and add channel dimension (for Conv2D layers)\n",
        "x_train_mnist = np.expand_dims(x_train_mnist, axis=-1)\n",
        "x_train_noisy = np.expand_dims(x_train_noisy, axis=-1)\n",
        "\n",
        "# Define and train an autoencoder model for denoising\n",
        "input_layer = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "autoencoder.fit(x_train_noisy, x_train_mnist, epochs=10, batch_size=128)\n",
        "\n",
        "# Extract the encoder part from the trained autoencoder\n",
        "encoder_model = Model(inputs=input_layer, outputs=encoded)\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(x_train_fashion, y_train_fashion), _ = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train_fashion = x_train_fashion / 255.0\n",
        "\n",
        "# Reshape input data and add channel dimension\n",
        "x_train_fashion = np.expand_dims(x_train_fashion, axis=-1)\n",
        "\n",
        "# Build a neural network for Fashion MNIST using the pretrained encoder\n",
        "input_layer_fashion = Input(shape=(28, 28, 1))\n",
        "encoded_features = encoder_model(input_layer_fashion)\n",
        "x = Dense(64, activation='relu')(tf.keras.layers.Flatten()(encoded_features))\n",
        "output_layer_fashion = Dense(10, activation='softmax')(x)\n",
        "\n",
        "classifier_model = Model(inputs=input_layer_fashion, outputs=output_layer_fashion)\n",
        "\n",
        "# Compile the classifier model\n",
        "classifier_model.compile(optimizer='adam',\n",
        "                         loss='sparse_categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "classifier_model.summary()\n",
        "\n",
        "# Train the classifier model (using Fashion MNIST data)\n",
        "classifier_model.fit(x_train_fashion, y_train_fashion, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKTtxIenJ3IR",
        "outputId": "0429db65-9080-420d-c9c7-a76517d791b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 81s 171ms/step - loss: 0.1337\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 77s 165ms/step - loss: 0.0832\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 80s 171ms/step - loss: 0.0793\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 77s 163ms/step - loss: 0.0774\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 74s 158ms/step - loss: 0.0763\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 74s 157ms/step - loss: 0.0755\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 74s 157ms/step - loss: 0.0748\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 71s 152ms/step - loss: 0.0743\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 73s 155ms/step - loss: 0.0738\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 74s 158ms/step - loss: 0.0734\n",
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " model_10 (Functional)       (None, 7, 7, 32)          9568      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1568)              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 64)                100416    \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 110634 (432.16 KB)\n",
            "Trainable params: 110634 (432.16 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 36s 19ms/step - loss: 0.4155 - accuracy: 0.8526\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 36s 19ms/step - loss: 0.2849 - accuracy: 0.8973\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.2424 - accuracy: 0.9124\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.2156 - accuracy: 0.9202\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.1949 - accuracy: 0.9286\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.1767 - accuracy: 0.9337\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.1605 - accuracy: 0.9400\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.1454 - accuracy: 0.9457\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.1312 - accuracy: 0.9512\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.1207 - accuracy: 0.9554\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dc368db95a0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster Optimizers\n",
        "# Momentum Optimization\n"
      ],
      "metadata": {
        "id": "wzBeVtlYJ79S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(x_train_fashion, y_train_fashion), _ = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train_fashion = x_train_fashion / 255.0\n",
        "\n",
        "# Reshape input data and add channel dimension\n",
        "x_train_fashion = np.expand_dims(x_train_fashion, axis=-1)\n",
        "\n",
        "# Build a neural network for Fashion MNIST using the pretrained encoder\n",
        "input_layer_fashion = Input(shape=(28, 28, 1))\n",
        "x = Flatten()(input_layer_fashion)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "output_layer_fashion = Dense(10, activation='softmax')(x)\n",
        "\n",
        "classifier_model = Model(inputs=input_layer_fashion, outputs=output_layer_fashion)\n",
        "\n",
        "# Compile the classifier model with Adam optimizer and default momentum\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss='sparse_categorical_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "classifier_model.summary()\n",
        "\n",
        "# Train the classifier model with faster optimizer (Adam with momentum)\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "classifier_model.fit(x_train_fashion, y_train_fashion,\n",
        "                      epochs=epochs,\n",
        "                      batch_size=batch_size)"
      ],
      "metadata": {
        "id": "nJ6cvVd6J-1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503a7f83-5445-4c03-95bb-5571951f835d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                50240     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50890 (198.79 KB)\n",
            "Trainable params: 50890 (198.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 20s 10ms/step - loss: 0.5207 - accuracy: 0.8189\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3915 - accuracy: 0.8624\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3528 - accuracy: 0.8722\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3288 - accuracy: 0.8815\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3093 - accuracy: 0.8872\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2968 - accuracy: 0.8908\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2857 - accuracy: 0.8943\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2756 - accuracy: 0.8982\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2697 - accuracy: 0.9004\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2602 - accuracy: 0.9040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x790593dcff40>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}