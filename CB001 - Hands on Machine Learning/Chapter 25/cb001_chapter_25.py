# -*- coding: utf-8 -*-
"""CB001 - Chapter 25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MKYj2CPGIyLktEUKxbO5LDA11OzTYs2u

#Batch Normalization
"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist

# Load MNIST dataset
(x_train, y_train), (x_val, y_val) = mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train, x_val = x_train / 255.0, x_val / 255.0

# Flatten the input data
x_train = x_train.reshape(x_train.shape[0], -1)
x_val = x_val.reshape(x_val.shape[0], -1)

# Define the model architecture with BatchNormalization
input_layer = Input(shape=(784,))
x = Dense(128, activation='relu')(input_layer)
x = BatchNormalization()(x)
x = Dense(64, activation='relu')(x)
x = BatchNormalization()(x)
output_layer = Dense(10, activation='softmax')(x)

model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Print model summary
model.summary()

# Train the model
batch_size = 32
epochs = 10

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          validation_data=(x_val, y_val))

# Evaluate the model
loss, accuracy = model.evaluate(x_val, y_val)
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

"""#ELU Activation"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist

# Load MNIST dataset
(x_train, y_train), (x_val, y_val) = mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train, x_val = x_train / 255.0, x_val / 255.0

# Flatten the input data
x_train = x_train.reshape(x_train.shape[0], -1)
x_val = x_val.reshape(x_val.shape[0], -1)

# Define the model architecture with BatchNormalization and ELU activations
input_layer = Input(shape=(784,))
x = Dense(128)(input_layer)
x = BatchNormalization()(x)
x = Activation('elu')(x)
x = Dense(64)(x)
x = BatchNormalization()(x)
x = Activation('elu')(x)
output_layer = Dense(10, activation='softmax')(x)

model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Print model summary
model.summary()

# Train the model
batch_size = 32
epochs = 10

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          validation_data=(x_val, y_val))

# Evaluate the model
loss, accuracy = model.evaluate(x_val, y_val)
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

"""#Fast RELU"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist

# Load MNIST dataset
(x_train, y_train), (x_val, y_val) = mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train, x_val = x_train / 255.0, x_val / 255.0

# Flatten the input data
x_train = x_train.reshape(x_train.shape[0], -1)
x_val = x_val.reshape(x_val.shape[0], -1)

# Define the FastReLU activation function
def fast_relu(x):
    return tf.maximum(0.01 * x, x)

# Define the model architecture with BatchNormalization and FastReLU activations
input_layer = Input(shape=(784,))
x = Dense(128)(input_layer)
x = BatchNormalization()(x)
x = Activation(fast_relu)(x)
x = Dense(64)(x)
x = BatchNormalization()(x)
x = Activation(fast_relu)(x)
output_layer = Dense(10, activation='softmax')(x)

model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Print model summary
model.summary()

# Train the model
batch_size = 32
epochs = 10

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          validation_data=(x_val, y_val))

# Evaluate the model
loss, accuracy = model.evaluate(x_val, y_val)
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

"""# Gradient Clipping

"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist

# Load MNIST dataset
(x_train, y_train), (x_val, y_val) = mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train, x_val = x_train / 255.0, x_val / 255.0

# Flatten the input data
x_train = x_train.reshape(x_train.shape[0], -1)
x_val = x_val.reshape(x_val.shape[0], -1)

# Define the FastReLU activation function
def fast_relu(x):
    return tf.maximum(0.01 * x, x)

# Define the model architecture with BatchNormalization and FastReLU activations
input_layer = Input(shape=(784,))
x = Dense(128)(input_layer)
x = BatchNormalization()(x)
x = Activation(fast_relu)(x)
x = Dense(64)(x)
x = BatchNormalization()(x)
x = Activation(fast_relu)(x)
output_layer = Dense(10, activation='softmax')(x)

model = Model(inputs=input_layer, outputs=output_layer)

# Define optimizer with gradient clipping
optimizer = tf.keras.optimizers.Adam(clipvalue=0.5)  # Set clipvalue as desired threshold

# Compile the model with custom optimizer
model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Print model summary
model.summary()

# Train the model
batch_size = 32
epochs = 10

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          validation_data=(x_val, y_val))

# Evaluate the model
loss, accuracy = model.evaluate(x_val, y_val)
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

"""#Reusing Pretrained Layers with Transfer Learning"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist, fashion_mnist

# Load MNIST dataset for pre-training
(x_train_mnist, y_train_mnist), _ = mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train_mnist = x_train_mnist / 255.0

# Flatten the input data
x_train_mnist = x_train_mnist.reshape(x_train_mnist.shape[0], -1)

# Define the pre-trained model (using MNIST data)
input_layer_mnist = Input(shape=(784,))
x = Dense(128)(input_layer_mnist)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Dense(64)(x)
x = BatchNormalization()(x)
output_layer_mnist = Activation('softmax')(x)

pretrained_model_mnist = Model(inputs=input_layer_mnist, outputs=output_layer_mnist)

# Compile the pre-trained model (for demonstration purposes)
pretrained_model_mnist.compile(optimizer='adam',
                               loss='sparse_categorical_crossentropy',
                               metrics=['accuracy'])

# Train the pre-trained model (using MNIST data)
pretrained_model_mnist.fit(x_train_mnist, y_train_mnist, epochs=5, batch_size=32)

# Load Fashion MNIST dataset
(x_train_fashion, y_train_fashion), _ = fashion_mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train_fashion = x_train_fashion / 255.0

# Flatten the input data
x_train_fashion = x_train_fashion.reshape(x_train_fashion.shape[0], -1)

# Freeze the lower layers of the pre-trained model
for layer in pretrained_model_mnist.layers:
    layer.trainable = False

# Replace the top layers for Fashion MNIST
output_layer_fashion = Dense(10, activation='softmax')(pretrained_model_mnist.layers[-3].output)
transfer_model = Model(inputs=pretrained_model_mnist.input, outputs=output_layer_fashion)

# Compile the transfer model
transfer_model.compile(optimizer='adam',
                       loss='sparse_categorical_crossentropy',
                       metrics=['accuracy'])

# Print model summary
transfer_model.summary()

# Train the transfer model (using Fashion MNIST data)
transfer_model.fit(x_train_fashion, y_train_fashion, epochs=10, batch_size=32)

"""#Unsupervised Pretraining"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist, fashion_mnist

# Load MNIST dataset for unsupervised pretraining
(x_train_mnist, _), _ = mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train_mnist = x_train_mnist / 255.0

# Flatten the input data
x_train_mnist = x_train_mnist.reshape(x_train_mnist.shape[0], -1)

# Define and train an autoencoder model on MNIST
input_layer = Input(shape=(784,))
encoded = Dense(128, activation='relu')(input_layer)
encoded = Dense(64, activation='relu')(encoded)
decoded = Dense(128, activation='relu')(encoded)
decoded = Dense(784, activation='sigmoid')(decoded)

autoencoder = Model(inputs=input_layer, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.fit(x_train_mnist, x_train_mnist, epochs=10, batch_size=32)

# Extract the encoder part from the trained autoencoder
encoder_model = Model(inputs=input_layer, outputs=encoded)

# Load Fashion MNIST dataset
(x_train_fashion, y_train_fashion), _ = fashion_mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train_fashion = x_train_fashion / 255.0

# Flatten the input data
x_train_fashion = x_train_fashion.reshape(x_train_fashion.shape[0], -1)

# Build a neural network for Fashion MNIST using the pretrained encoder
input_layer_fashion = Input(shape=(784,))
encoded_features = encoder_model(input_layer_fashion)
x = Dense(64, activation='relu')(encoded_features)
x = BatchNormalization()(x)
output_layer_fashion = Dense(10, activation='softmax')(x)

classifier_model = Model(inputs=input_layer_fashion, outputs=output_layer_fashion)

# Compile the classifier model
classifier_model.compile(optimizer='adam',
                         loss='sparse_categorical_crossentropy',
                         metrics=['accuracy'])

# Print model summary
classifier_model.summary()

# Train the classifier model (using Fashion MNIST data)
classifier_model.fit(x_train_fashion, y_train_fashion, epochs=10, batch_size=32)

"""# Pretraining on an Auxiliary Task"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist, fashion_mnist
import numpy as np

# Load MNIST dataset for unsupervised pretraining
(x_train_mnist, _), _ = mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train_mnist = x_train_mnist / 255.0

# Add random noise to the MNIST images for denoising task
noise_factor = 0.2
x_train_noisy = x_train_mnist + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train_mnist.shape)
x_train_noisy = np.clip(x_train_noisy, 0., 1.)  # Clip values to ensure they are within [0, 1]

# Reshape input data and add channel dimension (for Conv2D layers)
x_train_mnist = np.expand_dims(x_train_mnist, axis=-1)
x_train_noisy = np.expand_dims(x_train_noisy, axis=-1)

# Define and train an autoencoder model for denoising
input_layer = Input(shape=(28, 28, 1))
x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)
x = MaxPooling2D((2, 2), padding='same')(x)
x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
encoded = MaxPooling2D((2, 2), padding='same')(x)

x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)
x = UpSampling2D((2, 2))(x)
x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D((2, 2))(x)
decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

autoencoder = Model(inputs=input_layer, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

autoencoder.fit(x_train_noisy, x_train_mnist, epochs=10, batch_size=128)

# Extract the encoder part from the trained autoencoder
encoder_model = Model(inputs=input_layer, outputs=encoded)

# Load Fashion MNIST dataset
(x_train_fashion, y_train_fashion), _ = fashion_mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train_fashion = x_train_fashion / 255.0

# Reshape input data and add channel dimension
x_train_fashion = np.expand_dims(x_train_fashion, axis=-1)

# Build a neural network for Fashion MNIST using the pretrained encoder
input_layer_fashion = Input(shape=(28, 28, 1))
encoded_features = encoder_model(input_layer_fashion)
x = Dense(64, activation='relu')(tf.keras.layers.Flatten()(encoded_features))
output_layer_fashion = Dense(10, activation='softmax')(x)

classifier_model = Model(inputs=input_layer_fashion, outputs=output_layer_fashion)

# Compile the classifier model
classifier_model.compile(optimizer='adam',
                         loss='sparse_categorical_crossentropy',
                         metrics=['accuracy'])

# Print model summary
classifier_model.summary()

# Train the classifier model (using Fashion MNIST data)
classifier_model.fit(x_train_fashion, y_train_fashion, epochs=10, batch_size=32)

"""# Faster Optimizers
# Momentum Optimization

"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import fashion_mnist
import numpy as np

# Load Fashion MNIST dataset
(x_train_fashion, y_train_fashion), _ = fashion_mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train_fashion = x_train_fashion / 255.0

# Reshape input data and add channel dimension
x_train_fashion = np.expand_dims(x_train_fashion, axis=-1)

# Build a neural network for Fashion MNIST using the pretrained encoder
input_layer_fashion = Input(shape=(28, 28, 1))
x = Flatten()(input_layer_fashion)
x = Dense(64, activation='relu')(x)
output_layer_fashion = Dense(10, activation='softmax')(x)

classifier_model = Model(inputs=input_layer_fashion, outputs=output_layer_fashion)

# Compile the classifier model with Adam optimizer and default momentum
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)

classifier_model.compile(optimizer=optimizer,
                         loss='sparse_categorical_crossentropy',
                         metrics=['accuracy'])

# Print model summary
classifier_model.summary()

# Train the classifier model with faster optimizer (Adam with momentum)
batch_size = 32
epochs = 10

classifier_model.fit(x_train_fashion, y_train_fashion,
                      epochs=epochs,
                      batch_size=batch_size)