{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Customizing Models and Training Algorithms"
      ],
      "metadata": {
        "id": "6WvPoJt6QXlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Loss Functions"
      ],
      "metadata": {
        "id": "pOn3uCJWQgWG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T0J9G5lQJk7",
        "outputId": "a84e324a-b4bb-46bc-9b4f-7f6f7373d32c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 21s 9ms/step - loss: 0.0062 - accuracy: 0.9206 - val_loss: 0.0028 - val_accuracy: 0.9642\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0030 - accuracy: 0.9613 - val_loss: 0.0020 - val_accuracy: 0.9753\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0022 - accuracy: 0.9731 - val_loss: 0.0019 - val_accuracy: 0.9748\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0017 - accuracy: 0.9786 - val_loss: 0.0017 - val_accuracy: 0.9772\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0014 - accuracy: 0.9831 - val_loss: 0.0017 - val_accuracy: 0.9780\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0012 - accuracy: 0.9853 - val_loss: 0.0017 - val_accuracy: 0.9772\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 9.9154e-04 - accuracy: 0.9881 - val_loss: 0.0016 - val_accuracy: 0.9788\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 8.7678e-04 - accuracy: 0.9897 - val_loss: 0.0015 - val_accuracy: 0.9805\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 7.9564e-04 - accuracy: 0.9905 - val_loss: 0.0018 - val_accuracy: 0.9762\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 6.5606e-04 - accuracy: 0.9924 - val_loss: 0.0015 - val_accuracy: 0.9815\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0018 - accuracy: 0.9769\n",
            "Test Accuracy: 0.9768999814987183\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28)) / 255.0\n",
        "test_images = test_images.reshape((10000, 28 * 28)) / 255.0\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, 10)\n",
        "\n",
        "# Define the custom Huber loss function\n",
        "def huber_fn(y_true, y_pred):\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < 1.0\n",
        "    squared_loss = tf.square(error) / 2\n",
        "    linear_loss = tf.abs(error) - 0.5\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "# Build a simple neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(28 * 28,)),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model using the custom loss function\n",
        "model.compile(loss=huber_fn, optimizer=Nadam(), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving and Loading Models That Contain Custom Components\n"
      ],
      "metadata": {
        "id": "lhnWW2K8STuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class HuberLoss(keras.losses.Loss):\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < self.threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}"
      ],
      "metadata": {
        "id": "pmzGlSVASve_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model using HuberLoss\n",
        "model.compile(loss=HuberLoss(threshold=1.5), optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "QRrUFnGWSxIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model with custom loss function\n",
        "model.save(\"my_model_with_custom_loss.h5\")\n",
        "\n",
        "# Load the model with custom loss function\n",
        "loaded_model = keras.models.load_model(\n",
        "    \"my_model_with_custom_loss.h5\",\n",
        "    custom_objects={\"HuberLoss\": HuberLoss}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NEAzNt1S0O_",
        "outputId": "d76f87d9-79bd-41dd-af4f-0b3511254867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmyaaUGqS5KG",
        "outputId": "6f33ac21-06de-4dac-af1e-9fd5e6f89361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0064 - accuracy: 0.9170 - val_loss: 0.0028 - val_accuracy: 0.9658\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0031 - accuracy: 0.9604 - val_loss: 0.0024 - val_accuracy: 0.9702\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0023 - accuracy: 0.9716 - val_loss: 0.0019 - val_accuracy: 0.9750\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0018 - accuracy: 0.9781 - val_loss: 0.0017 - val_accuracy: 0.9772\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0015 - accuracy: 0.9823 - val_loss: 0.0019 - val_accuracy: 0.9757\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0012 - accuracy: 0.9851 - val_loss: 0.0019 - val_accuracy: 0.9760\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0011 - accuracy: 0.9875 - val_loss: 0.0016 - val_accuracy: 0.9777\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 9.5409e-04 - accuracy: 0.9885 - val_loss: 0.0016 - val_accuracy: 0.9777\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 8.1189e-04 - accuracy: 0.9905 - val_loss: 0.0016 - val_accuracy: 0.9778\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 7.2333e-04 - accuracy: 0.9918 - val_loss: 0.0016 - val_accuracy: 0.9803\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b59a59fac80>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = loaded_model.evaluate(test_images, test_labels)\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "_PUR-XRnS8ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Activation Functions, Initializers, Regularizers, and Constraints"
      ],
      "metadata": {
        "id": "q52US3XmTBHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "x6GCkuw3n6_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_softplus(z):\n",
        "    return tf.math.log(tf.exp(z) + 1.0)\n",
        "\n",
        "def my_glorot_initializer(shape, dtype=tf.float32):\n",
        "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
        "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
        "\n",
        "def my_l1_regularizer(weights):\n",
        "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
        "\n",
        "def my_positive_weights(weights):\n",
        "    return tf.nn.relu(weights)"
      ],
      "metadata": {
        "id": "JsdBqTFvn8Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, weights):\n",
        "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"factor\": self.factor}"
      ],
      "metadata": {
        "id": "wY418cjyn9R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation=my_softplus, kernel_initializer=my_glorot_initializer, kernel_regularizer=my_l1_regularizer),\n",
        "    keras.layers.Dense(10, activation=my_positive_weights)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load MNIST data\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59bmGQEUn-jr",
        "outputId": "07dc8249-4f96-437d-aa4d-99e8501f7661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 2.8905 - accuracy: 0.3518 - val_loss: 2.1043 - val_accuracy: 0.3973\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 2.0455 - accuracy: 0.4124 - val_loss: 2.0142 - val_accuracy: 0.4168\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 1.9775 - accuracy: 0.4298 - val_loss: 1.9575 - val_accuracy: 0.4346\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 1.9409 - accuracy: 0.4400 - val_loss: 1.9364 - val_accuracy: 0.4429\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 1.9173 - accuracy: 0.4473 - val_loss: 1.9144 - val_accuracy: 0.4494\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 1.9013 - accuracy: 0.4516 - val_loss: 1.8992 - val_accuracy: 0.4477\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 1.8898 - accuracy: 0.4543 - val_loss: 1.8877 - val_accuracy: 0.4601\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 1.8800 - accuracy: 0.4570 - val_loss: 1.8747 - val_accuracy: 0.4594\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 1.8725 - accuracy: 0.4594 - val_loss: 1.8615 - val_accuracy: 0.4595\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 1.8664 - accuracy: 0.4606 - val_loss: 1.8646 - val_accuracy: 0.4624\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 1.8646 - accuracy: 0.4624\n",
            "Test accuracy: 0.46239998936653137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Metrics\n"
      ],
      "metadata": {
        "id": "13gQRjM4oFDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris_data = load_iris()\n",
        "X = iris_data['data']\n",
        "y = iris_data['target']\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the Huber loss function\n",
        "def create_huber(threshold):\n",
        "    def huber_fn(y_true, y_pred):\n",
        "        error = tf.cast(y_true, dtype=tf.float32) - y_pred\n",
        "        is_small_error = tf.abs(error) <= threshold\n",
        "        small_error_loss = tf.square(error) / 2\n",
        "        large_error_loss = threshold * (tf.abs(error) - 0.5 * threshold)\n",
        "        return tf.where(is_small_error, small_error_loss, large_error_loss)\n",
        "    return huber_fn\n",
        "\n",
        "# Define the HuberMetric class\n",
        "class HuberMetric(keras.metrics.Metric):\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.threshold = threshold\n",
        "        self.huber_fn = create_huber(threshold)\n",
        "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
        "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.cast(y_true, dtype=tf.float32)  # Convert y_true to float32\n",
        "        metric = self.huber_fn(y_true, y_pred)\n",
        "        self.total.assign_add(tf.reduce_sum(metric))\n",
        "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}\n",
        "\n",
        "# Build the model with HuberMetric\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu', input_shape=(4,)),\n",
        "    keras.layers.Dense(3, activation='softmax')  # 3 output units for 3 classes with softmax activation\n",
        "])\n",
        "\n",
        "# Compile the model with HuberMetric as a custom metric\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy', HuberMetric()])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=20, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc, test_huber = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test accuracy: {test_acc}, Test Huber: {test_huber}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpIT52rioO5a",
        "outputId": "71b64961-3523-4278-b847-893d74a20a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 3s 152ms/step - loss: 1.0649 - accuracy: 0.3000 - huber_metric_5: 1.4219 - val_loss: 0.9827 - val_accuracy: 0.6667 - val_huber_metric_5: 1.5396\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 0.9772 - accuracy: 0.6917 - huber_metric_5: 1.4224 - val_loss: 0.8943 - val_accuracy: 0.8667 - val_huber_metric_5: 1.5409\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.8964 - accuracy: 0.7750 - huber_metric_5: 1.4248 - val_loss: 0.8145 - val_accuracy: 0.8667 - val_huber_metric_5: 1.5439\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.8246 - accuracy: 0.8000 - huber_metric_5: 1.4294 - val_loss: 0.7441 - val_accuracy: 0.8667 - val_huber_metric_5: 1.5486\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.7610 - accuracy: 0.8083 - huber_metric_5: 1.4357 - val_loss: 0.6827 - val_accuracy: 0.8667 - val_huber_metric_5: 1.5547\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.7060 - accuracy: 0.8083 - huber_metric_5: 1.4432 - val_loss: 0.6292 - val_accuracy: 0.8667 - val_huber_metric_5: 1.5620\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.6593 - accuracy: 0.8167 - huber_metric_5: 1.4516 - val_loss: 0.5826 - val_accuracy: 0.8667 - val_huber_metric_5: 1.5701\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 0.6165 - accuracy: 0.8083 - huber_metric_5: 1.4613 - val_loss: 0.5425 - val_accuracy: 0.8667 - val_huber_metric_5: 1.5784\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 0.5820 - accuracy: 0.8167 - huber_metric_5: 1.4698 - val_loss: 0.5072 - val_accuracy: 0.8667 - val_huber_metric_5: 1.5869\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.5500 - accuracy: 0.8167 - huber_metric_5: 1.4794 - val_loss: 0.4768 - val_accuracy: 0.8667 - val_huber_metric_5: 1.5947\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.5238 - accuracy: 0.8167 - huber_metric_5: 1.4871 - val_loss: 0.4503 - val_accuracy: 0.9000 - val_huber_metric_5: 1.6023\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.4999 - accuracy: 0.8250 - huber_metric_5: 1.4950 - val_loss: 0.4275 - val_accuracy: 0.9000 - val_huber_metric_5: 1.6091\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.4792 - accuracy: 0.8250 - huber_metric_5: 1.5014 - val_loss: 0.4072 - val_accuracy: 0.9000 - val_huber_metric_5: 1.6153\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.4604 - accuracy: 0.8250 - huber_metric_5: 1.5077 - val_loss: 0.3891 - val_accuracy: 0.9000 - val_huber_metric_5: 1.6209\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 0.4437 - accuracy: 0.8417 - huber_metric_5: 1.5132 - val_loss: 0.3731 - val_accuracy: 0.9000 - val_huber_metric_5: 1.6260\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.4293 - accuracy: 0.8417 - huber_metric_5: 1.5180 - val_loss: 0.3586 - val_accuracy: 0.9000 - val_huber_metric_5: 1.6307\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.4155 - accuracy: 0.8500 - huber_metric_5: 1.5224 - val_loss: 0.3455 - val_accuracy: 0.9000 - val_huber_metric_5: 1.6349\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.4026 - accuracy: 0.8417 - huber_metric_5: 1.5264 - val_loss: 0.3336 - val_accuracy: 0.9000 - val_huber_metric_5: 1.6388\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 0.3913 - accuracy: 0.8417 - huber_metric_5: 1.5299 - val_loss: 0.3224 - val_accuracy: 0.9000 - val_huber_metric_5: 1.6422\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 0.3807 - accuracy: 0.8417 - huber_metric_5: 1.5334 - val_loss: 0.3120 - val_accuracy: 0.9000 - val_huber_metric_5: 1.6454\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3120 - accuracy: 0.9000 - huber_metric_5: 1.6454\n",
            "Test accuracy: 0.8999999761581421, Test Huber: 1.6453816890716553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Layers\n"
      ],
      "metadata": {
        "id": "fY-jov0IoW2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define the model using Lambda layer\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Lambda(lambda x: tf.square(x))  # Custom Lambda layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "ouWFPJ8cog6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393075bc-0861-461e-aef4-dbbe964dc98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 20.5242 - accuracy: 0.0000e+00 - val_loss: 20.5419 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 20.5242 - accuracy: 0.0000e+00 - val_loss: 20.5419 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 20.5242 - accuracy: 0.0000e+00 - val_loss: 20.5419 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 20.5243 - accuracy: 0.0000e+00 - val_loss: 20.5419 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 20.5242 - accuracy: 0.0000e+00 - val_loss: 20.5419 - val_accuracy: 0.0000e+00\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 20.5419 - accuracy: 0.0000e+00\n",
            "Test accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Custom Dense Layer\n",
        "class MyDense(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
        "            initializer=\"glorot_normal\")\n",
        "        self.bias = self.add_weight(\n",
        "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
        "        super().build(batch_input_shape)\n",
        "\n",
        "    def call(self, X):\n",
        "        return self.activation(tf.matmul(X, self.kernel) + self.bias)\n",
        "\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
        "\n",
        "# Build the model using MyDense layer\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    MyDense(128, activation='relu'),  # Custom MyDense layer\n",
        "    keras.layers.Dense(10, activation='softmax')  # Standard Dense layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgt5hEU0pf2y",
        "outputId": "6e06f5b0-21f8-42d5-b6a3-3296748173d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2537 - accuracy: 0.9279 - val_loss: 0.1347 - val_accuracy: 0.9594\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1110 - accuracy: 0.9666 - val_loss: 0.1016 - val_accuracy: 0.9711\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0767 - accuracy: 0.9770 - val_loss: 0.0790 - val_accuracy: 0.9756\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0574 - accuracy: 0.9824 - val_loss: 0.0793 - val_accuracy: 0.9768\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0450 - accuracy: 0.9859 - val_loss: 0.0722 - val_accuracy: 0.9784\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0722 - accuracy: 0.9784\n",
            "Test accuracy: 0.9783999919891357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Custom Multi-layer\n",
        "class MyMultiLayer(keras.layers.Layer):\n",
        "    def call(self, X):\n",
        "        X1, X2 = X\n",
        "        return [X1 + X2, X1 * X2, X1 / (X2 + 1e-6)]  # Avoid division by zero\n",
        "\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        b1, b2 = batch_input_shape\n",
        "        return [b1, b1, b1]\n",
        "\n",
        "# Example of using MyMultiLayer in a model\n",
        "input1 = keras.layers.Input(shape=(32,))\n",
        "input2 = keras.layers.Input(shape=(32,))\n",
        "output1, output2, output3 = MyMultiLayer()([input1, input2])\n",
        "\n",
        "# Define a model\n",
        "model = keras.Model(inputs=[input1, input2], outputs=[output1, output2, output3])\n",
        "\n",
        "# Compile and train the model (using dummy data for illustration)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Generate dummy data\n",
        "import numpy as np\n",
        "X1_train = np.random.rand(100, 32)\n",
        "X2_train = np.random.rand(100, 32)\n",
        "Y1_train = X1_train + X2_train\n",
        "Y2_train = X1_train * X2_train\n",
        "Y3_train = X1_train / (X2_train + 1e-6)  # Avoid division by zero\n",
        "\n",
        "# Train the model\n",
        "model.fit([X1_train, X2_train], [Y1_train, Y2_train, Y3_train], epochs=5, batch_size=32)\n",
        "\n",
        "# Evaluate the model (using dummy test data)\n",
        "X1_test = np.random.rand(10, 32)\n",
        "X2_test = np.random.rand(10, 32)\n",
        "Y_test = model.predict([X1_test, X2_test])\n",
        "\n",
        "# Example output shape and results\n",
        "print(\"Output shape:\", [y.shape for y in Y_test])\n",
        "print(\"Sample output values (sum, product, division):\")\n",
        "for i in range(3):\n",
        "    print(f\"Output {i+1}:\", Y_test[i][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ln4mSOGp1t6",
        "outputId": "be7a8fc1-94b3-4046-8e06-60a8382d65f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "4/4 [==============================] - 1s 6ms/step - loss: 3.2074e-12 - my_multi_layer_loss: 1.7272e-15 - my_multi_layer_1_loss: 2.6022e-16 - my_multi_layer_2_loss: 3.2054e-12\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 3.2074e-12 - my_multi_layer_loss: 1.7272e-15 - my_multi_layer_1_loss: 2.6022e-16 - my_multi_layer_2_loss: 3.2054e-12\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 3.2074e-12 - my_multi_layer_loss: 1.7272e-15 - my_multi_layer_1_loss: 2.6022e-16 - my_multi_layer_2_loss: 3.2054e-12\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 3.2074e-12 - my_multi_layer_loss: 1.7272e-15 - my_multi_layer_1_loss: 2.6022e-16 - my_multi_layer_2_loss: 3.2054e-12\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 3.2074e-12 - my_multi_layer_loss: 1.7272e-15 - my_multi_layer_1_loss: 2.6022e-16 - my_multi_layer_2_loss: 3.2054e-12\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "Output shape: [(10, 32), (10, 32), (10, 32)]\n",
            "Sample output values (sum, product, division):\n",
            "Output 1: [1.630543   1.1262277  0.47610322 1.2463055  0.74668926 0.48023456\n",
            " 0.7119241  0.94170237 0.23165612 1.2453083  1.569416   1.8655006\n",
            " 0.7891532  0.89506185 0.7191317  1.660857   0.28375828 1.3665178\n",
            " 1.0255071  1.343291   0.98137903 0.6980038  0.5659173  0.7543691\n",
            " 1.1924261  0.5777253  0.4703397  1.1542491  1.0432991  1.1581447\n",
            " 0.7528569  0.2781361 ]\n",
            "Output 2: [6.5674269e-01 2.7614722e-01 5.4910850e-02 3.7556002e-01 3.7344426e-02\n",
            " 4.7779799e-02 1.2328238e-01 1.2184826e-01 8.1435535e-03 3.3717093e-01\n",
            " 6.0669076e-01 8.6843503e-01 1.1619223e-01 1.9669445e-01 1.0449504e-01\n",
            " 6.8429524e-01 5.3789670e-04 3.7589762e-01 2.5738049e-01 3.7499791e-01\n",
            " 1.4495532e-01 6.4828455e-02 5.7900809e-02 1.3252018e-01 3.3203053e-01\n",
            " 3.0046592e-02 4.5442361e-02 3.0798295e-01 1.4091270e-01 2.4275988e-01\n",
            " 1.3603429e-01 1.6827384e-02]\n",
            "Output 3: [1.24515247e+00 2.12187624e+00 1.42752457e+00 6.93095803e-01\n",
            " 1.28517542e+01 4.14542198e-01 7.17550457e-01 5.08109283e+00\n",
            " 4.36038828e+00 2.12992048e+00 1.27636015e+00 9.18052852e-01\n",
            " 3.02968192e+00 7.63864756e-01 3.90909433e-01 1.19250309e+00\n",
            " 1.47607758e+02 3.87566894e-01 1.33946085e+00 4.17682737e-01\n",
            " 4.41776705e+00 5.32761431e+00 3.10487181e-01 1.70914090e+00\n",
            " 1.69101298e+00 1.11146055e-01 2.46193194e+00 1.75656271e+00\n",
            " 5.54404211e+00 3.21406627e+00 6.66759610e-01 2.12710977e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class MyGaussianNoise(keras.layers.Layer):\n",
        "    def __init__(self, stddev, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def call(self, X, training=None):\n",
        "        if training:\n",
        "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
        "            return X + noise\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        return batch_input_shape"
      ],
      "metadata": {
        "id": "xlbDug8qqCpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Create a simple model with MyGaussianNoise layer\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(784,)),\n",
        "    MyGaussianNoise(stddev=0.1),  # Add Gaussian noise during training\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784) / 255.0\n",
        "x_test = x_test.reshape(-1, 784) / 255.0\n",
        "\n",
        "# Train the model with noisy inputs\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJV2E7W8qATi",
        "outputId": "6b13b384-8fe0-4299-d759-f2b99f2b54ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2779 - accuracy: 0.9209 - val_loss: 0.1224 - val_accuracy: 0.9680\n",
            "Epoch 2/5\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.1226 - accuracy: 0.9639 - val_loss: 0.0954 - val_accuracy: 0.9737\n",
            "Epoch 3/5\n",
            "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0849 - accuracy: 0.9749 - val_loss: 0.1026 - val_accuracy: 0.9718\n",
            "Epoch 4/5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0651 - accuracy: 0.9806 - val_loss: 0.0922 - val_accuracy: 0.9725\n",
            "Epoch 5/5\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0505 - accuracy: 0.9845 - val_loss: 0.0821 - val_accuracy: 0.9743\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0778 - accuracy: 0.9762\n",
            "Test accuracy: 0.9761999845504761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class ResidualBlock(keras.layers.Layer):\n",
        "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
        "                                           kernel_initializer=\"he_normal\")\n",
        "                       for _ in range(n_layers)]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.hidden:\n",
        "            Z = layer(Z)\n",
        "        return inputs + Z\n",
        "\n",
        "# Example of using ResidualBlock in a model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='elu', kernel_initializer='he_normal'),\n",
        "    ResidualBlock(n_layers=2, n_neurons=128),  # Custom ResidualBlock layer\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scYh2smRqJQx",
        "outputId": "0d65850b-3562-41ae-9957-626a414762c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2290 - accuracy: 0.9298 - val_loss: 0.1234 - val_accuracy: 0.9613\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1073 - accuracy: 0.9667 - val_loss: 0.1028 - val_accuracy: 0.9686\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0768 - accuracy: 0.9761 - val_loss: 0.0937 - val_accuracy: 0.9713\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0602 - accuracy: 0.9799 - val_loss: 0.1082 - val_accuracy: 0.9698\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0488 - accuracy: 0.9839 - val_loss: 0.0996 - val_accuracy: 0.9721\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0996 - accuracy: 0.9721\n",
            "Test accuracy: 0.972100019454956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Model"
      ],
      "metadata": {
        "id": "R2TBvEsyqg0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define the ResidualBlock class\n",
        "class ResidualBlock(keras.layers.Layer):\n",
        "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
        "                                           kernel_initializer=\"he_normal\")\n",
        "                       for _ in range(n_layers)]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.hidden:\n",
        "            Z = layer(Z)\n",
        "        return inputs + Z\n",
        "\n",
        "# Define the ResidualRegressor model\n",
        "class ResidualRegressor(keras.models.Model):\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
        "                                          kernel_initializer=\"he_normal\")\n",
        "        self.block1 = ResidualBlock(2, 30)\n",
        "        self.block2 = ResidualBlock(2, 30)\n",
        "        self.out = keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = self.hidden1(inputs)\n",
        "        for _ in range(3):  # Apply block1 three times\n",
        "            Z = self.block1(Z)\n",
        "        for _ in range(3):  # Apply block2 three times\n",
        "            Z = self.block2(Z)\n",
        "        return self.out(Z)\n",
        "\n",
        "# Load and preprocess the California Housing dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing_data = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing_data.data, housing_data.target, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and compile the ResidualRegressor model\n",
        "model = ResidualRegressor(output_dim=1)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test loss (MSE): {test_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZOQElGBqhO1",
        "outputId": "b73087e6-4a68-4d6b-964b-3b36cd1fd88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "465/465 [==============================] - 3s 4ms/step - loss: 21.8768 - val_loss: 6.7047\n",
            "Epoch 2/10\n",
            "465/465 [==============================] - 1s 3ms/step - loss: 2.3281 - val_loss: 3.9290\n",
            "Epoch 3/10\n",
            "465/465 [==============================] - 3s 5ms/step - loss: 3.6640 - val_loss: 0.8711\n",
            "Epoch 4/10\n",
            "465/465 [==============================] - 3s 6ms/step - loss: 0.7780 - val_loss: 0.7196\n",
            "Epoch 5/10\n",
            "465/465 [==============================] - 4s 8ms/step - loss: 1.7409 - val_loss: 0.7081\n",
            "Epoch 6/10\n",
            "465/465 [==============================] - 2s 5ms/step - loss: 0.7386 - val_loss: 0.5964\n",
            "Epoch 7/10\n",
            "465/465 [==============================] - 3s 6ms/step - loss: 2.0020 - val_loss: 1.3000\n",
            "Epoch 8/10\n",
            "465/465 [==============================] - 2s 5ms/step - loss: 0.9411 - val_loss: 0.6719\n",
            "Epoch 9/10\n",
            "465/465 [==============================] - 1s 3ms/step - loss: 2.8853 - val_loss: 0.6098\n",
            "Epoch 10/10\n",
            "465/465 [==============================] - 2s 4ms/step - loss: 0.6665 - val_loss: 0.5103\n",
            "129/129 [==============================] - 0s 3ms/step - loss: 0.4798\n",
            "Test loss (MSE): 0.47980964183807373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Losses and Metrics Based on Model Internals\n"
      ],
      "metadata": {
        "id": "6RspMEwWqsAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define the ReconstructingRegressor model\n",
        "class ReconstructingRegressor(keras.models.Model):\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden = [keras.layers.Dense(64, activation=\"relu\",\n",
        "                                           kernel_initializer=\"he_normal\") for _ in range(4)]\n",
        "        self.out = keras.layers.Dense(output_dim)\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        n_inputs = batch_input_shape[-1]\n",
        "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
        "        super().build(batch_input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.hidden:\n",
        "            Z = layer(Z)\n",
        "        reconstruction = self.reconstruct(Z)\n",
        "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
        "        self.add_loss(0.1 * recon_loss)  # Adjust regularization factor\n",
        "\n",
        "        return self.out(Z)\n"
      ],
      "metadata": {
        "id": "FDv6PgIVqp7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Computing Gradients Using Autodiff\n"
      ],
      "metadata": {
        "id": "4xExt333rYCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(w1, w2):\n",
        " return 3 * w1 ** 2 + 2 * w1 * w2"
      ],
      "metadata": {
        "id": "iRCO_l7RrYbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2 = 5, 3\n",
        "eps = 1e-6\n",
        "(f(w1 + eps, w2) - f(w1, w2)) / eps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNwj0ngRraIB",
        "outputId": "9a63f7c8-e938-4bee-d0bb-75f7fff6fd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36.000003007075065"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(f(w1, w2 + eps) - f(w1, w2)) / eps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAW-3ojhrgTR",
        "outputId": "410a28e5-c98f-4c05-ec0b-7ef4834b780a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.000000003174137"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
        "with tf.GradientTape() as tape:\n",
        " z = f(w1, w2)\n",
        "gradients = tape.gradient(z, [w1, w2])"
      ],
      "metadata": {
        "id": "VgXWckgbrh5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradients"
      ],
      "metadata": {
        "id": "OpD7WgSurjrI",
        "outputId": "5a29aa8c-36c7-48eb-c13a-53e79c964917",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}