# -*- coding: utf-8 -*-
"""CB001 - Chapter 23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wKd8mbtj7a1bfstfq1E43GDhk19v9jd5

#Using a MLP for Regression Tasks

Method 1:
"""

import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing

# Load California Housing dataset
data = fetch_california_housing()

# Create a DataFrame from the dataset
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target

# Display the dataset information
print(df.head())
print(df.describe())

from sklearn.preprocessing import StandardScaler

# Separate features and target variable
X = df.drop('target', axis=1)
y = df['target']

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error

# Initialize MLPRegressor
mlp = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam',
                   max_iter=500, random_state=42)

# Train the MLP model
mlp.fit(X_train, y_train)

# Predict on the test set
y_pred = mlp.predict(X_test)

# Calculate Mean Squared Error (MSE) on the test set
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse:.4f}")

# Visualize predictions against actual values
import matplotlib.pyplot as plt

plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs. Predicted (MLP Regression)')
plt.show()

"""Method 2:"""

import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load California Housing dataset
data = fetch_california_housing()

# Create a DataFrame from the dataset
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target

# Separate features and target variable
X = df.drop('target', axis=1)
y = df['target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Define the MLP model
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(1)  # Output layer (1 neuron for regression output)
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Display the model architecture
model.summary()

# Train the model
history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
mse = model.evaluate(X_test_scaled, y_test)
print(f"Mean Squared Error (MSE) on Test Set: {mse:.4f}")

import matplotlib.pyplot as plt

# Plot training history
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

"""#Wide and Deep Neural Networks

#Use Concat

For connecting a part of the input directly to the output
"""

import tensorflow as tf

# Load MNIST dataset
mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize pixel values to range [0, 1]
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Reshape input images to 1D arrays (flatten)
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

# Convert labels to one-hot encoding
y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=10)
y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=10)

from tensorflow import keras

# Define input layer
input_layer = keras.layers.Input(shape=(X_train_flat.shape[1],))

# Hidden layers
hidden1 = keras.layers.Dense(30, activation='relu')(input_layer)
hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)

# Concatenate input with output of hidden2
concat_layer = keras.layers.Concatenate()([input_layer, hidden2])

# Output layer (softmax for multi-class classification)
output_layer = keras.layers.Dense(10, activation='softmax')(concat_layer)

# Create the model
model = keras.models.Model(inputs=[input_layer], outputs=[output_layer])

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Display model summary
model.summary()

# Train the model
history = model.fit(X_train_flat, y_train_onehot, epochs=30, batch_size=32, validation_split=0.2)

# Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test_flat, y_test_onehot)
print(f"Accuracy on Test Set: {accuracy:.4f}")

import matplotlib.pyplot as plt

# Plot training history
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

#Mount the google drive connection to our dataset
from google.colab import drive
drive.mount('/content/drive')

# Plot the model architecture
from tensorflow.keras.utils import plot_model

plot_model(model, to_file='/content/drive/My Drive/AI/mnist_mlp_concat_model.png', show_shapes=True, show_layer_names=True)

"""For dividing the data into subsets of features:"""

import tensorflow as tf
from tensorflow import keras

# Load MNIST dataset
mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize pixel values to range [0, 1]
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Reshape input images to 1D arrays (flatten)
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

# Convert labels to one-hot encoding
y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=10)
y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=10)

# Define input layers
input_A = keras.layers.Input(shape=[X_train_flat.shape[1]])  # Input for flattened image data
input_B = keras.layers.Input(shape=[10])  # Input for one-hot encoded labels (10 classes)

# Hidden layers for input_A (image data)
hidden1_A = keras.layers.Dense(128, activation='relu')(input_A)
hidden2_A = keras.layers.Dense(64, activation='relu')(hidden1_A)

# Concatenate input_A with input_B
concat_layer = keras.layers.concatenate([hidden2_A, input_B])

# Output layer (softmax for multi-class classification)
output_layer = keras.layers.Dense(10, activation='softmax')(concat_layer)

# Create the model
model = keras.models.Model(inputs=[input_A, input_B], outputs=[output_layer])

# Compile the model (use categorical crossentropy and accuracy metric for classification)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Display model summary
model.summary()

# Train the model
history = model.fit([X_train_flat, y_train_onehot], y_train_onehot, epochs=30, batch_size=32, validation_split=0.2)

# Evaluate the model on the test set
loss, accuracy = model.evaluate([X_test_flat, y_test_onehot], y_test_onehot)
print(f"Accuracy on Test Set: {accuracy:.4f}")

# Plot training history (accuracy)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

# Plot training history (loss)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# Plot the model architecture
plot_model(model, to_file='/content/drive/My Drive/AI/mnist_mlp_concat_model.png', show_shapes=True, show_layer_names=True)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.utils import plot_model
import matplotlib.pyplot as plt

# Load MNIST dataset
mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize pixel values to range [0, 1]
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Define input features subsets
X_train_first5 = X_train[:, :5, :].reshape(X_train.shape[0], -1)  # First 5 pixels of each image (flattened)
X_train_rest = X_train[:, 5:, :].reshape(X_train.shape[0], -1)    # Rest of the pixels (after the first 5) of each image (flattened)

X_test_first5 = X_test[:, :5, :].reshape(X_test.shape[0], -1)     # First 5 pixels of each test image (flattened)
X_test_rest = X_test[:, 5:, :].reshape(X_test.shape[0], -1)        # Rest of the pixels (after the first 5) of each test image (flattened)

# Convert labels to one-hot encoding
y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=10)
y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=10)

# Define input layers
input_A = keras.layers.Input(shape=[X_train_first5.shape[1]])  # Input for the first 5 pixels
input_B = keras.layers.Input(shape=[X_train_rest.shape[1]])    # Input for the rest of the pixels

# Hidden layers for input_B
hidden1_B = keras.layers.Dense(128, activation='relu')(input_B)
hidden2_B = keras.layers.Dense(64, activation='relu')(hidden1_B)

# Concatenate input_A with output of hidden2_B
concat_layer = keras.layers.concatenate([input_A, hidden2_B])

# Output layer (softmax for multi-class classification)
output_layer = keras.layers.Dense(10, activation='softmax')(concat_layer)

# Create the model
model = keras.models.Model(inputs=[input_A, input_B], outputs=[output_layer])

# Compile the model (use categorical crossentropy and accuracy metric for classification)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Display model summary
model.summary()

# Plot the model architecture
plot_model(model, to_file='/content/drive/My Drive/AI/mnist_mlp_concat_model.png', show_shapes=True, show_layer_names=True)

# Train the model
history = model.fit([X_train_first5, X_train_rest], y_train_onehot, epochs=30, batch_size=32, validation_split=0.2)

# Evaluate the model on the test set
loss, accuracy = model.evaluate([X_test_first5, X_test_rest], y_test_onehot)
print(f"Accuracy on Test Set: {accuracy:.4f}")

# Plot training history (accuracy)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

# Plot training history (loss)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# Plot the model architecture
plot_model(model, to_file='/content/drive/My Drive/AI/mnist_mlp_concat_model.png', show_shapes=True, show_layer_names=True)

"""For adding an extra_output layer:"""

import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt

# Load MNIST dataset
mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize pixel values to range [0, 1]
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Reshape input images to 1D arrays (flatten)
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

# Split input features into two subsets
X_train_first5 = X_train[:, :5, :].reshape(X_train.shape[0], -1)  # First 5 pixels of each image (flattened)
X_train_rest = X_train[:, 5:, :].reshape(X_train.shape[0], -1)    # Rest of the pixels (after the first 5) of each image (flattened)

X_test_first5 = X_test[:, :5, :].reshape(X_test.shape[0], -1)     # First 5 pixels of each test image (flattened)
X_test_rest = X_test[:, 5:, :].reshape(X_test.shape[0], -1)        # Rest of the pixels (after the first 5) of each test image (flattened)

# Convert labels to one-hot encoding
y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=10)
y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=10)

# Define input layers
input_A = keras.layers.Input(shape=[X_train_first5.shape[1]])  # Input for the first 5 pixels
input_B = keras.layers.Input(shape=[X_train_rest.shape[1]])    # Input for the rest of the pixels

# Hidden layers for input_B
hidden1_B = keras.layers.Dense(128, activation='relu')(input_B)
hidden2_B = keras.layers.Dense(64, activation='relu')(hidden1_B)

# Concatenate input_A with output of hidden2_B
concat_layer = keras.layers.concatenate([input_A, hidden2_B])

# Main output for classification (softmax activation)
output_layer = keras.layers.Dense(10, activation='softmax', name='main_output')(concat_layer)

# Auxiliary output for regression (linear activation)
aux_output = keras.layers.Dense(1, name='aux_output')(hidden2_B)

# Create the model with multiple outputs
model = keras.models.Model(inputs=[input_A, input_B], outputs=[output_layer, aux_output])

# Compile the model with multiple losses and loss weights
model.compile(loss=['categorical_crossentropy', 'mse'],
              loss_weights=[0.9, 0.1],
              optimizer='sgd',
              metrics={'main_output': 'accuracy'})

# Display model summary
model.summary()

# Train the model
history = model.fit([X_train_first5, X_train_rest], [y_train_onehot, y_train.reshape(-1, 1)],
                    epochs=20,
                    validation_data=([X_test_first5, X_test_rest], [y_test_onehot, y_test.reshape(-1, 1)]))

# Evaluate the model on the test set
losses = model.evaluate([X_test_first5, X_test_rest], [y_test_onehot, y_test.reshape(-1, 1)])

# Extract individual losses
total_loss = losses[0]  # Total loss (combination of both losses)
categorical_crossentropy_loss = losses[1]  # Loss from categorical cross-entropy
mse_loss = losses[2]  # Loss from mean squared error

print(f"Total Loss: {total_loss:.4f}")
print(f"Categorical Cross-Entropy Loss: {categorical_crossentropy_loss:.4f}")
print(f"Mean Squared Error Loss: {mse_loss:.4f}")

# Ensure X_test is reshaped correctly to match the model's input requirements
X_test_first5 = X_test[:, :5, :].reshape(X_test.shape[0], -1)
X_test_rest = X_test[:, 5:, :].reshape(X_test.shape[0], -1)

# Example: Use first 5 test images for prediction
X_new_first5 = X_test_first5[:5, :]  # First 5 test images (reshape as needed)
X_new_rest = X_test_rest[:5, :]      # Remaining test images corresponding to X_new_first5

# Verify shapes
print(f"X_new_first5 shape: {X_new_first5.shape}")
print(f"X_new_rest shape: {X_new_rest.shape}")

# Predict using the model
y_pred_main, y_pred_aux = model.predict([X_new_first5, X_new_rest])

# Display predictions
print("Predicted Classes (Main Output):")
print(tf.argmax(y_pred_main, axis=1).numpy())
print("Predicted Auxiliary Values:")
print(y_pred_aux.flatten())

# Plot training history (accuracy)
plt.plot(history.history['main_output_accuracy'], label='Training Accuracy (Main Output)')
plt.plot(history.history['val_main_output_accuracy'], label='Validation Accuracy (Main Output)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy (Main Output)')
plt.legend()
plt.show()

# Plot training history (loss)
plt.plot(history.history['main_output_loss'], label='Training Loss (Main Output)')
plt.plot(history.history['val_main_output_loss'], label='Validation Loss (Main Output)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss (Main Output)')
plt.legend()
plt.show()

# Plot the model architecture
plot_model(model, to_file='/content/drive/My Drive/AI/mnist_mlp_concat_model.png', show_shapes=True, show_layer_names=True)

"""# Building Dynamic Models Using the Subclassing API"""

import tensorflow as tf
from tensorflow import keras

# Create synthetic dataset
import numpy as np
np.random.seed(0)
X_train_A = np.random.rand(1000, 5)  # Example feature set A
X_train_B = np.random.rand(1000, 5)  # Example feature set B
y_train_main = np.random.rand(1000, 1)  # Example main target
y_train_aux = np.random.rand(1000, 1)  # Example auxiliary target

# Define the WideAndDeepModel class
class WideAndDeepModel(keras.models.Model):
    def __init__(self, units=30, activation="relu", **kwargs):
        super().__init__(**kwargs)  # handles standard args (e.g., name)
        self.hidden1 = keras.layers.Dense(units, activation=activation)
        self.hidden2 = keras.layers.Dense(units, activation=activation)
        self.main_output = keras.layers.Dense(1)
        self.aux_output = keras.layers.Dense(1)

    def call(self, inputs):
        input_A, input_B = inputs
        hidden1 = self.hidden1(input_B)
        hidden2 = self.hidden2(hidden1)
        concat = keras.layers.concatenate([input_A, hidden2])
        main_output = self.main_output(concat)
        aux_output = self.aux_output(hidden2)
        return main_output, aux_output

# Instantiate the model
model = WideAndDeepModel()

# Compile the model
model.compile(optimizer='adam',
              loss=['mse', 'mse'],  # Specify two losses for main and auxiliary outputs
              loss_weights=[0.9, 0.1],  # Weighted contribution of each loss
              metrics=['mae'])  # Optional: metrics to monitor during training

# Train the model
history = model.fit([X_train_A, X_train_B],  # Input is a list of A and B
                    [y_train_main, y_train_aux],  # Output is a list of main and auxiliary targets
                    epochs=10,
                    batch_size=32,
                    validation_split=0.2)  # Optional: Split data into training and validation

"""#Save the model:"""

# Save the entire model to a HDF5 file
model.save('my_model.h5')

# Load the saved model
loaded_model = tf.keras.models.load_model('my_model.h5')

# Evaluate the restored model
y_pred_main, y_pred_aux = loaded_model.predict([X_new_first5, X_new_rest])

# Display predictions
print("Predicted Classes (Main Output):")
print(tf.argmax(y_pred_main, axis=1).numpy())
print("Predicted Auxiliary Values:")
print(y_pred_aux.flatten())

"""# Using Callbacks"""

import tensorflow as tf
from tensorflow import keras

# Load MNIST dataset
mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Preprocess the data
train_images = train_images / 255.0
test_images = test_images / 255.0

# Build the model
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Custom Callback to print validation loss over training loss ratio
class PrintValTrainRatioCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        print("\nval/train: {:.2f}".format(logs["val_loss"] / logs["loss"]))

# Define callbacks
checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.h5", save_best_only=True)
print_val_train_ratio_cb = PrintValTrainRatioCallback()
early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

# Train the model
history = model.fit(train_images, train_labels, epochs=20,
                    validation_data=(test_images, test_labels),
                    callbacks=[checkpoint_cb, print_val_train_ratio_cb, early_stopping_cb])

# Evaluate the model
test_loss, test_acc = model.evaluate(test_images, test_labels)
print("\nTest accuracy:", test_acc)

"""# Visualization Using TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
!pip install tensorflow
import tensorflow as tf
from tensorflow import keras
import os
from datetime import datetime

# Load MNIST dataset
mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Preprocess the data
train_images = train_images / 255.0
test_images = test_images / 255.0

# Build the model
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Set up TensorBoard
logdir = os.path.join("logs", datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)

# Train the model with TensorBoard callback
model.fit(train_images, train_labels, epochs=10,
          validation_data=(test_images, test_labels),
          callbacks=[tensorboard_callback])

# Access TensorBoard within the notebook
# %load_ext tensorboard
# %tensorboard --logdir logs