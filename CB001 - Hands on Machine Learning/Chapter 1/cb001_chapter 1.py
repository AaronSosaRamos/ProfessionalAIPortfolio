# -*- coding: utf-8 -*-
"""C001 - Chapter1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-VEsRhfqkEonAuIbOCCpAXXOwE8xSw07

# C001 - Hands on Machine Learning

# Chapter 1 - Fundamentals of ML

# Supervised Learning

k-Nearest Neighbors
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
X = iris.data
y = iris.target

y

"""Scatterplot - Recovered from: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html"""

import matplotlib.pyplot as plt

_, ax = plt.subplots()
scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)
ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])
_ = ax.legend(
    scatter.legend_elements()[0], iris.target_names, loc="lower right", title="Classes"
)

"""

*   Train dataset (60%)
*   Validation dataset (20%)
*   Test dataset (20%)


"""

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

"""k value (Find the optimal value of k for this context):"""

k_range = list(range(1, 31))

cv_scores = []

from sklearn.model_selection import cross_val_score

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

"""Optimal k"""

optimal_k = k_range[cv_scores.index(max(cv_scores))]
print("Optimal k =", optimal_k)

plt.plot(k_range, cv_scores)
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Cross-Validation Accuracy')
plt.title('kNN Cross-Validation Performance')
plt.show()

"""The Iris Dataset is overfitting the kNN Algo."""

knn = KNeighborsClassifier(n_neighbors=optimal_k)
knn.fit(X_train, y_train)

"""Loss function => Accuracy"""

y_val_pred = knn.predict(X_val)
val_accuracy = accuracy_score(y_val, y_val_pred)
print("Validation Accuracy with k =", optimal_k, ":", val_accuracy)

y_test_pred = knn.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Test Accuracy with k =", optimal_k, ":", test_accuracy)

"""Test KNN algo. with another dataset - Apple Quality Classification



"""

#Mount the google drive connection to our dataset
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd
df = pd.read_csv('/content/drive/My Drive/AI/Project 4/dataset/apple_quality.csv')

df.drop(df.tail(1).index, inplace=True)
df.drop(columns=['A_id'], inplace=True)
df

df.info()

df['Acidity'] = pd.to_numeric(df['Acidity'], errors='coerce')
print(df['Acidity'].dtype)

df.info()

"""Scatterplot of Quality vs Size"""

plt.figure(figsize=(8, 6))
plt.scatter(df['Size'], df['Quality'])

plt.xlabel('Size')
plt.ylabel('Quality')
plt.title('Scatter Plot of Quality vs Size')

plt.show()

"""Define the Pipeline"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

df['Quality_encoded'] = label_encoder.fit_transform(df['Quality'])
df.groupby('Quality')['Quality_encoded'].unique().reset_index()

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('Quality').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

df.drop(columns=['Quality'], inplace=True)
df

X = df.drop("Quality_encoded", axis=1)
y = df["Quality_encoded"]

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

"""k value (Find the optimal value of k for this context):"""

k_range = list(range(1, 31))

cv_scores = []

from sklearn.model_selection import cross_val_score

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

"""Optimal k"""

optimal_k = k_range[cv_scores.index(max(cv_scores))]
print("Optimal k =", optimal_k)

plt.plot(k_range, cv_scores)
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Cross-Validation Accuracy')
plt.title('kNN Cross-Validation Performance')
plt.show()

knn_apple_quality = KNeighborsClassifier(n_neighbors=optimal_k)
knn.fit(X_train, y_train)

y_val_pred = knn.predict(X_val)
val_accuracy = accuracy_score(y_val, y_val_pred)
print("Validation Accuracy with k =", optimal_k, ":", val_accuracy)

y_test_pred = knn.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Test Accuracy with k =", optimal_k, ":", test_accuracy)

df_to_test = {
    "Size": 2.451215,
    "Weight": 3.672134,
    "Sweetness": 6.125521,
    "Crunchiness": 2.661234,
    "Juiciness": 3.512312,
    "Ripeness": 4.124552,
    "Acidity": 4.156312
}

df_test = pd.DataFrame([df_to_test])

final_prediction = knn.predict(df_test)
print(f"Final prediction: {final_prediction[0]}")