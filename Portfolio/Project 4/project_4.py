# -*- coding: utf-8 -*-
"""Project 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DWHNLCau6zefv7Ss1Vde8K4vqqDh6W7o

# Project 4 - Wilfredo Aaron Sosa Ramos

Connect to Google Drive
"""

#Mount the google drive connection to our dataset
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""#Phase 1: Extract

Load the dataset
"""

import pandas as pd
df = pd.read_csv('/content/drive/My Drive/AI/Project 4/dataset/apple_quality.csv')

"""Check our dataset"""

df

"""Remove the last row"""

df.drop(df.tail(1).index, inplace=True)

df

"""Metadata:"""

df.info()

df.iloc[400]

"""Statistical info:"""

df.describe()

"""Null values:"""

df.isnull().sum()

df.shape

"""Statistic graphics:

Histogram of Size:
"""

import matplotlib.pyplot as plt

plt.hist(df['Size'], bins=20, color='skyblue', edgecolor='black')
plt.xlabel('Size')
plt.ylabel('Frequency')
plt.title('Histogram of Size')
plt.show()

"""Histogram of Weight:"""

plt.hist(df['Weight'], bins=20, color='lightgreen', edgecolor='black')
plt.xlabel('Weight')
plt.ylabel('Frequency')
plt.title('Histogram of Weight')
plt.show()

"""Histogram of Sweetness:"""

plt.hist(df['Sweetness'], bins=20, color='salmon', edgecolor='black')
plt.xlabel('Sweetness')
plt.ylabel('Frequency')
plt.title('Histogram of Sweetness')
plt.show()

"""Histogram of Crunchiness:"""

plt.hist(df['Crunchiness'], bins=20, color='orange', edgecolor='black')
plt.xlabel('Crunchiness')
plt.ylabel('Frequency')
plt.title('Histogram of Crunchiness')
plt.show()

"""Histogram of Juiciness:"""

plt.hist(df['Juiciness'], bins=20, color='lightcoral', edgecolor='black')
plt.xlabel('Juiciness')
plt.ylabel('Frequency')
plt.title('Histogram of Juiciness')
plt.show()

"""Histogram of Ripeness:"""

plt.hist(df['Ripeness'], bins=20, color='lightblue', edgecolor='black')
plt.xlabel('Ripeness')
plt.ylabel('Frequency')
plt.title('Histogram of Ripeness')
plt.show()

"""Bar Chart of Quality:"""

quality_values = df['Quality'].unique()

plt.bar(range(len(quality_values)), df['Quality'].value_counts(), color=['blue','red'])

plt.xlabel('Quality')
plt.ylabel('Frequency')
plt.title('Bar Chart of Quality')

plt.xticks(range(len(quality_values)), quality_values)

plt.show()

"""# 1. Look at the Big Picture

Frame the Problem:

*   Classify apples according to their Quality => Supervised Learning
*   Analyse unseen groups of apples => Unsupervised Learning

Select a Performance Measure:
*   Accuracy

Check the assumptions:

*   Dataset's label (Target column) is based in categories.

# 2. Get the Data

Create the Workspace:

*   We are using the Google Colab Jupyter notebooks (Scientific Python distribution)

Download the Data:
"""

df.head()

"""Take a Quick Look at the Data Structure:"""

df["Quality"].value_counts()

df.describe()

import matplotlib.pyplot as plt
df.hist(bins=50, figsize=(20,15))
plt.show()

"""# 3. Discover and Visualize the Data to Gain Insights"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

numerical_features = ['Size', 'Weight', 'Sweetness', 'Crunchiness', 'Juiciness', 'Ripeness']
for feature in numerical_features:
    sns.boxplot(x='Quality', y=feature, data=df)
    plt.title(f"{feature} vs Quality")
    plt.show()

""" Looking for Correlations

"""

df.drop("A_id", axis=1, inplace=True)

corr_matrix = df.corr()

corr_matrix

from pandas.plotting import scatter_matrix
 scatter_matrix(df, figsize=(12, 8))

"""# 4. Prepare the Data for Machine Learning Algorithms

Data Cleaning: This dataset is cleaned

Handling Text and Categorical Attributes
"""

df.info()

df['Acidity'] = df['Acidity'].astype('float64')

df.info()

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

encoder_pipeline = Pipeline([
    ('onehot_encoder', OneHotEncoder(categories='auto', sparse=False, drop='first'))
])

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

def build_pipeline(columns_to_encode):
    column_transformer = ColumnTransformer([
        ("onehot_encoder", OneHotEncoder(categories='auto', sparse=False, drop='first'), columns_to_encode)
    ], remainder='passthrough')

    pipeline = Pipeline([
        ('column_transformer', column_transformer)
    ])

    return pipeline

columns_to_encode = ['Quality']

encode_pipeline = build_pipeline(columns_to_encode)

df_encoded = encode_pipeline.fit_transform(df)

onehot_encoder = encode_pipeline.named_steps['column_transformer'].named_transformers_['onehot_encoder']
encoded_feature_names = onehot_encoder.get_feature_names_out(input_features=columns_to_encode)

all_feature_names = list(encoded_feature_names) + list(df.columns.drop(columns_to_encode))

df_encoded = pd.DataFrame(df_encoded, columns=all_feature_names)

df_encoded.head()

df_encoded.info()

onehot_encoder.categories_

"""# 5. Select and Train a Model

Training and Evaluating on the Training Set

Divide the data (60 - 20 - 20 model)

LightGBM
"""

import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X = df_encoded.drop(columns=['Quality_good'])
y = df_encoded['Quality_good']

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

"""LightGBM params."""

params = {
    'objective': 'binary',            # Binary classification
    'metric': 'binary_error',        # Evaluation metric: binary error
    'verbosity': -1,                  # No output while training
    'early_stopping_rounds': 50,      # Early stopping rounds
    'verbose_eval': 100
}

train_data = lgb.Dataset(X_train, label=y_train)
val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

"""Cross-Validation for Best Number of Rounds"""

cv_results = lgb.cv(params, train_data, num_boost_round=1000)

best_n_rounds = len(cv_results['valid binary_error-mean'])

lgb_clf = lgb.train(params, train_data, num_boost_round=best_n_rounds, valid_sets=[train_data, val_data])

"""Loss Function => Accuracy"""

y_val_pred = lgb_clf.predict(X_val, num_iteration=lgb_clf.best_iteration)
val_accuracy = accuracy_score(y_val, (y_val_pred > 0.5).astype(int))
print("Validation accuracy:", val_accuracy)

y_test_pred = lgb_clf.predict(X_test, num_iteration=lgb_clf.best_iteration)
test_accuracy = accuracy_score(y_test, (y_test_pred > 0.5).astype(int))
print("Test accuracy:", test_accuracy)

"""XGBoost"""

import xgboost as xgb

"""Define the XGBoost parameters"""

params = {
    'objective': 'binary:logistic',  # Binary classification with logistic regression
    'eval_metric': 'error',          # Evaluation metric: classification error
    'verbosity': 2,                  # No output while training
    'early_stopping_rounds': 50,     # Early stopping rounds
    'verbose_eval': 100
}

dtrain = xgb.DMatrix(X_train, label=y_train)
dval = xgb.DMatrix(X_val, label=y_val)

"""Cross-Validation with KFold"""

from sklearn.model_selection import KFold

kf = KFold(n_splits=5, shuffle=True, random_state=42)

cv_results = xgb.cv(params, dtrain, num_boost_round=1000, folds=kf, verbose_eval=100)

optimal_num_boost_rounds = len(cv_results)

bst = xgb.train(params, dtrain, num_boost_round=optimal_num_boost_rounds, evals=[(dtrain, 'train'), (dval, 'val')])

y_val_pred = bst.predict(dval)
val_accuracy = accuracy_score(y_val, (y_val_pred > 0.5).astype(int))
print("Validation accuracy:", val_accuracy)

dtest = xgb.DMatrix(X_test)
y_test_pred = bst.predict(dtest)
test_accuracy = accuracy_score(y_test, (y_test_pred > 0.5).astype(int))
print("Test accuracy:", test_accuracy)

"""We can save our models with Python Picke Module

# 6. Fine-Tune Your Model

Grid Search (For a smaller amount of possible combinations between hyperparams.)
"""

from sklearn.svm import SVC

svm_classifier = SVC(kernel='linear', random_state=42)
svm_classifier.fit(X_train, y_train)

y_val_pred = svm_classifier.predict(X_val)
val_accuracy = accuracy_score(y_val, y_val_pred)
print("Validation Accuracy:", val_accuracy)

y_test_pred = svm_classifier.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Test Accuracy:", test_accuracy)

param_grid = {
    'C': [0.1, 1, 10, 100],  # Regularization parameter
    'kernel': ['linear', 'rbf', 'poly'],  # Kernel type
    'gamma': ['scale', 'auto']  # Kernel coefficient for 'rbf' and 'poly'
}

from sklearn.model_selection import GridSearchCV

grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy')

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
print("Best Parameters:", best_params)

best_svm_classifier = grid_search.best_estimator_

y_val_pred = best_svm_classifier.predict(X_val)
val_accuracy = accuracy_score(y_val, y_val_pred)
print("Validation Accuracy (with best parameters):", val_accuracy)

y_test_pred = best_svm_classifier.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Test Accuracy (with best parameters):", test_accuracy)

from sklearn.ensemble import RandomForestClassifier

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2']
}

rf_classifier = RandomForestClassifier(random_state=42)

grid_search_rf = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')

grid_search_rf.fit(X_train, y_train)

best_params_rf = grid_search_rf.best_params_
print("Best Parameters (Random Forest):", best_params_rf)

best_rf_classifier = grid_search_rf.best_estimator_

y_val_pred_rf = best_rf_classifier.predict(X_val)
val_accuracy_rf = accuracy_score(y_val, y_val_pred_rf)
print("Validation Accuracy (Random Forest - Best Parameters):", val_accuracy_rf)

y_test_pred_rf = best_rf_classifier.predict(X_test)
test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)
print("Test Accuracy (Random Forest - Best Parameters):", test_accuracy_rf)

"""Randomized Search (For a bigger amount of possible combinations between hyperparams. - A number of random combinatios) - RandomizedSearchCV

Ensemble Methods (Example with VotingRegressor)
"""

from sklearn.ensemble import VotingClassifier

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

voting_classifier = VotingClassifier(
    estimators=[('svm', best_svm_classifier), ('rf', best_rf_classifier)],
    voting='hard'
)

voting_classifier.fit(X_train, y_train)

"""Loss Function => Accuracy"""

y_val_pred = voting_classifier.predict(X_val)

val_accuracy = accuracy_score(y_val, y_val_pred)
print("Validation Accuracy (VotingClassifier):", val_accuracy)

y_test_pred = voting_classifier.predict(X_test)

test_accuracy = accuracy_score(y_test, y_test_pred)
print("Test Accuracy (VotingClassifier):", test_accuracy)

"""# 7. Launch, Monitor, and Maintain Your System

Deployment, DevOps (CI/CD) and automate processes

# 8. Try It Out!
"""

df_encoded.iloc[0]

data_test = {
    'Size': 5.612351,
    'Weight': 4.512312,
    'Sweetness': 1.512312,
    'Crunchiness': 6.121412,
    'Juiciness': 1.512312,
    'Ripeness': 7.121212,
    'Acidity': 4.412341
}

df_test = pd.DataFrame([data_test])

prediction = best_svm_classifier.predict(df_test)
print("Is quality good?: ",prediction[0])