# -*- coding: utf-8 -*-
"""Project 13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yDwgg9EfZ4L4BTYNQqQCbr8gK5kqvEa4

#Project 13 - Semi-Supervised Learning Regression - Wilfredo Aaron Sosa Ramos

#Google AI human-centered design approach (https://ai.google/responsibility/responsible-ai-practices/)

#Phase 1: Analysis
"""

#Mount the google drive connection to our dataset
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd
df = pd.read_csv('/content/drive/My Drive/AI/datasets/housing.csv')

df.head()

df.drop("Unnamed: 0", axis=1, inplace=True)

df.info()

df.shape

df.isnull().sum()

df.isna().sum()

"""#Phase 2: Transformation

Visualize the data
"""

import matplotlib.pyplot as plt
import seaborn as sns

def plot_charts(df):
  # Numeric features
  numeric_features = [col for col in df.columns if df[col].dtype in ['int64', 'float64']]
  for feature in numeric_features:
      plt.figure(figsize=(8, 6))
      sns.histplot(df[feature], kde=True, color='blue')
      plt.title(f'Distribution of {feature}')
      plt.xlabel(feature)
      plt.ylabel('Frequency')
      plt.show()

plot_charts(df)

df_without_target = df.drop("MedHouseVal", axis=1)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaler.fit(df_without_target)

df_without_target_scaled = scaler.transform(df_without_target)
df_without_target_scaled = pd.DataFrame(df_without_target_scaled, columns = df_without_target.columns)
df_without_target_scaled["MedHouseVal"] = df["MedHouseVal"]

train_df = df_without_target_scaled.sample(frac=0.5, random_state=42)
test_df = df_without_target_scaled.drop(train_df.index)

train_df.shape

test_df.shape

features = train_df.columns.tolist()
features.remove("MedHouseVal")

features

target = 'MedHouseVal'
# Preprocess the data
X_train, X_test = train_df[features], test_df[features]
y_train, y_test = train_df[target], test_df[target]

"""#Phase 3: Load

#Pseudo-Labelling
"""

def create_augmented_train(X, y, model, test, features, target, sample_rate):
    '''
    Create and return the augmented_train set that consists
    of pseudo-labeled and labeled data.
    '''
    num_of_samples = int(len(test) * sample_rate)
    # Train the model and creat the pseudo-labeles
    model.fit(X, y)
    pseudo_labeles = model.predict(test[features])
    # Add the pseudo-labeles to the test set
    augmented_test = test.copy(deep=True)
    augmented_test[target] = pseudo_labeles
    # Take a subset of the test set with pseudo-labeles and append in onto
    # the training set
    sampled_test = augmented_test.sample(n=num_of_samples)
    temp_train = pd.concat([X, y], axis=1)
    augemented_train = pd.concat([sampled_test, temp_train])

    # Shuffle the augmented dataset and return it
    return shuffle(augemented_train)

from sklearn.utils import shuffle
from sklearn.base import BaseEstimator, RegressorMixin
class PseudoLabeler(BaseEstimator, RegressorMixin):

    def __init__(self, model, test, features, target, sample_rate=0.2, seed=42):
        self.sample_rate = sample_rate
        self.seed = seed
        self.model = model
        self.model.seed = seed

        self.test = test
        self.features = features
        self.target = target

    def get_params(self, deep=True):
        return {
            "sample_rate": self.sample_rate,
            "seed": self.seed,
            "model": self.model,
            "test": self.test,
            "features": self.features,
            "target": self.target
        }
    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        return self

    def fit(self, X, y):
        if self.sample_rate > 0.0:
            augemented_train = self.__create_augmented_train(X, y)
            self.model.fit(
                augemented_train[self.features],
                augemented_train[self.target]
            )
        else:
            self.model.fit(X, y)

        return self
    def __create_augmented_train(self, X, y):
        num_of_samples = int(len(self.test) * self.sample_rate)

        # Train the model and creat the pseudo-labels
        self.model.fit(X, y)
        pseudo_labels = self.model.predict(self.test[self.features])

        # Add the pseudo-labels to the test set
        augmented_test = self.test.copy(deep=True)
        augmented_test[self.target] = pseudo_labels

        # Take a subset of the test set with pseudo-labels and append in onto
        # the training set
        sampled_test = augmented_test.sample(n=num_of_samples)
        temp_train = pd.concat([X, y], axis=1)
        augemented_train = pd.concat([sampled_test, temp_train])
        return shuffle(augemented_train)

    def predict(self, X):
        return self.model.predict(X)

    def get_model_name(self):
        return self.model.__class__.__name__

from sklearn.linear_model import SGDRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import LinearSVR
from sklearn.neural_network import MLPRegressor

sgdr_model = PseudoLabeler(
    SGDRegressor(max_iter=10000, random_state=42),
    test_df,
    features,
    target
)

rfr_model = PseudoLabeler(
    RandomForestRegressor(n_estimators=100, random_state=42),
    test_df,
    features,
    target
)

gbr_model = PseudoLabeler(
    GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),
    test_df,
    features,
    target
)

svr_model = PseudoLabeler(
    LinearSVR(epsilon=0.0, tol=0.0001, C=1.0, random_state=42),
    test_df,
    features,
    target
)

mlpr_model = PseudoLabeler(
    MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000, random_state=42),
    test_df,
    features,
    target
)

from sklearn.metrics import mean_squared_error
import numpy as np

def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

models = [sgdr_model, rfr_model, gbr_model, svr_model, mlpr_model]
model_names = ["SGD Regressor", "Random Forest Regressor", "Gradient Boosting Regressor", "Linear SVR", "MLP Regressor"]

for model, name in zip(models, model_names):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    rms_error = rmse(y_test, y_pred)
    print(f"{name}: Root Mean Squared Error: {rms_error:.4f}")