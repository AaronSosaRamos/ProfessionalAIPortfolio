# -*- coding: utf-8 -*-
"""Project 5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zz9ma-J0NXO7_4hUhioiu2JoDFLkqbyP

# Project 5 - Apple Clustering - Wilfredo Aaron Sosa Ramos

# Data Ingestion Technique

# Phase 1: Ingest

Connect to Google Drive
"""

#Mount the google drive connection to our dataset
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""Load the dataset"""

import pandas as pd
df = pd.read_csv('/content/drive/My Drive/AI/Project 5/dataset/apple_quality.csv')

df.head()

df.drop("A_id", axis=1, inplace=True)
df.drop(df.tail(1).index, inplace=True)

df.shape

df.isnull().sum()

df.isna().sum()

df.info()

"""Partition Clustering: Assume the x number of clusters and partition the data into that amount of divisions

Number of clusters is known before of performing clustering in Partition Clustering (Sundaram, 2021 => https://www.kaggle.com/code/gireeshs/complete-guide-to-clustering-techniques)

# Phase 2: Filter/Clean
"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
import pandas as pd

one_hot_cols = ['Quality']
scaled_cols = [col for col in df.columns if col not in one_hot_cols]

transformers = [
    ('one_hot', OneHotEncoder(), one_hot_cols),
    ('scaler', StandardScaler(), scaled_cols)
]

column_transformer = ColumnTransformer(transformers, remainder='passthrough')

transformed_data = column_transformer.fit_transform(df)

encoded_columns = column_transformer.named_transformers_['one_hot'].get_feature_names_out(input_features=one_hot_cols)
scaled_columns = scaled_cols
all_columns = list(encoded_columns) + scaled_columns

df_transformed = pd.DataFrame(transformed_data, columns=all_columns)

df_transformed.head()

df_transformed.info()

"""# Phase 3: Store (Load)

# K-Means Clustering
"""

from sklearn.cluster import KMeans

km = KMeans(init="random", n_clusters=3)
km.fit(df_transformed)

km.cluster_centers_

km.labels_

"""Solution of the optimal number of clusters (https://www.kaggle.com/code/gireeshs/complete-guide-to-clustering-techniques)"""

import matplotlib.pyplot as plt

distortions = []
K = range(1, 20)
for k in K:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(df_transformed)
    distortions.append(kmeanModel.inertia_)

plt.plot(K, distortions, 'bx-')
plt.xlabel('No of clusters (k)')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import KMeans

dimensions = ["Quality_good", "Size", "Juiciness"]

X = df_transformed[dimensions]

kmeans = KMeans(n_clusters=8, init='k-means++')
kmeans.fit(X)

centers = kmeans.cluster_centers_
labels = kmeans.labels_

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

for label in range(8):
    ax.scatter(X.loc[labels == label, dimensions[0]],
               X.loc[labels == label, dimensions[1]],
               X.loc[labels == label, dimensions[2]],
               label=f'Cluster {label + 1}')

ax.scatter(centers[:, 0], centers[:, 1], centers[:, 2], s=200, c='black', marker='*', label='Cluster Centers')

ax.set_xlabel(dimensions[0])
ax.set_ylabel(dimensions[1])
ax.set_zlabel(dimensions[2])
ax.set_title('3D Clustering of Data')

plt.legend()
plt.show()

"""# Limits of K-Means Clustering

It can only separate linear cluster boundaries

# Hierarchical Clustering

Bottom-up hierarchical clustering also known as agglomerative clustering. (https://www.kaggle.com/code/gireeshs/complete-guide-to-clustering-techniques)

A linkage is the separation between two clusters. There are:


*   Single linkage (Short distance)
*   Complete linkage (Large distance)
*   Average linkage (Between single and complete linkage)
"""

from sklearn.cluster import AgglomerativeClustering
hierarchical_clustering_model = AgglomerativeClustering().fit(df_transformed)
hierarchical_clustering_model

hierarchical_clustering_model.labels_

from scipy.cluster.hierarchy import dendrogram
import numpy as np

def plot_dendrogram(model, **kwargs):

    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)

    dendrogram(linkage_matrix, **kwargs)

hierarchical_clustering_model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)
hierarchical_clustering_model = hierarchical_clustering_model.fit(df_transformed)

plt.figure(figsize=(10, 6))
plt.title('Hierarchical Clustering Dendrogram')
plot_dendrogram(hierarchical_clustering_model, truncate_mode='level', p=3)
plt.xlabel("Number of points in node (or index of point if no parenthesis).")
plt.show()

"""# Spectral clustering

As Sundaram (2021) says:
*   Works on similarity graphs where each node represents an entity and weight on the edge
*   Consider the structure similar to a graph where all the nodes are connected to all other nodes with edges constituting of weights
*   If we want to split it into two clusters, clearly we want to want to eliminate the edges which has the lowest weight
"""

from sklearn.cluster import SpectralClustering
spectral_clustering_model = SpectralClustering(n_clusters=3, affinity='nearest_neighbors', assign_labels='kmeans').fit(df_transformed)
spectral_clustering_model

spectral_clustering_model.labels_

plt.figure(figsize=(8, 6))

plt.scatter(df_transformed['Size'], df_transformed['Juiciness'], c=spectral_clustering_model.labels_, cmap='viridis', s=50, alpha=0.7)
plt.title('Spectral Clustering with 3 clusters')
plt.xlabel('Size')
plt.ylabel('Juiciness')
plt.colorbar(label='Cluster')
plt.show()