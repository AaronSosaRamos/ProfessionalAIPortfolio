# -*- coding: utf-8 -*-
"""Project 9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W7_jhv1mhG3SlWgeWyYv6LI0gQ0ea2tB

#Project 9 - Dimensionality reduction - Wilfredo Aaron Sosa Ramos

#Data Engineering Lifecycle

#Phase 1: Ingestion
"""

#Mount the google drive connection to our dataset
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd
df = pd.read_csv('/content/drive/My Drive/AI/Project 9/dataset/abalone.csv')

df.head()

df.info()

df.describe()

df.shape

df.isnull().sum()

df.isna().sum()

df["Type"].unique()

"""# Phase 2: Transformation"""

import matplotlib.pyplot as plt
import seaborn as sns

def plot_charts(df):
  # Numeric features
  numeric_features = [col for col in df.columns if df[col].dtype in ['int64', 'float64']]
  for feature in numeric_features:
      plt.figure(figsize=(8, 6))
      sns.histplot(df[feature], kde=True, color='blue')
      plt.title(f'Distribution of {feature}')
      plt.xlabel(feature)
      plt.ylabel('Frequency')
      plt.show()

  # Categorical features
  categorical_features = [col for col in df.columns if df[col].dtype == 'object']
  for feature in categorical_features:
      plt.figure(figsize=(8, 6))
      sns.countplot(y=df[feature], order=df[feature].value_counts().index, palette='Set2')
      plt.title(f'Count of {feature}')
      plt.xlabel('Count')
      plt.ylabel(feature)
      plt.show()

plot_charts(df)

sns.pairplot(df, hue='ShellWeight', aspect=1.5)
plt.show()

numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_features = df.select_dtypes(include=['object']).columns.tolist()

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
import pandas as pd

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent'))
])

cat_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', cat_transformer, cat_features)
    ])

df_encoded = preprocessor.fit_transform(df)

new_column_names = preprocessor.named_transformers_['cat'] \
    .named_steps['encoder'].get_feature_names_out(input_features=cat_features)

new_column_names

columns = df.columns.tolist()

columns

columns.remove("Type")

columns

columns.extend(new_column_names.tolist())

columns

df_encoded = pd.DataFrame(df_encoded, columns=columns)

df_encoded.head()

X = df_encoded.drop("Type_F", axis=1)
y = df_encoded["Type_F"]

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

"""# Phase 3: Serving

# Principal Component Analysis (PCA)

Source: https://www.kaggle.com/code/pranaysingh25/dimensionality-reduction-visualization-mnist
"""

X_train_ = X_train.copy()

"""Compress data"""

import time
from sklearn.decomposition import PCA
pca = PCA(n_components = 0.95)
start = time.time()
X_reduced = pca.fit_transform(X_train)
end = time.time()

end - start

"""Number of important features (Only 2 features preserve the 95% of the data)"""

pca.n_components_

"""Decompress the data (5% of data is loss)"""

X_decompress = pca.inverse_transform(X_reduced)

X_reduced.size

X_reduced.shape

X_decompress.size

X_decompress.shape

"""Let's test it with RandomForestClassifier on Original Data"""

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=100, random_state = 42)
t0 = time.time()
rfc.fit(X_train_, y_train)
t1 = time.time()

t1-t0

"""Original data"""

rfc.score(X_test, y_test)

"""Let's test it with RandomForestClassifier on Reduced Data"""

rfc2 = RandomForestClassifier(n_estimators = 100, random_state = 42)
t0 = time.time()
rfc2.fit(X_reduced, y_train)
t1 = time.time()

t1-t0

X_test_reduced = pca.transform(X_test)
rfc2.score(X_test_reduced, y_test)

"""Let's test it with SoftmaxClassifier on Original Data"""

from sklearn.linear_model import LogisticRegression

log_clf = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', random_state = 42)
t0 = time.time()
log_clf.fit(X_train_, y_train)
t1 = time.time()

t1-t0

log_clf.score(X_test, y_test)

"""Let's test it with SoftmaxClassifier on Reduced Data"""

log_clf2 = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', random_state = 42)
t0 = time.time()
log_clf2.fit(X_reduced, y_train)
t1 = time.time()

t1-t0

log_clf2.score(X_test_reduced, y_test)

"""# Kernel PCA

Source: https://www.kaggle.com/code/goktugkoc/kernel-pca
"""

X_train2 = X_train_.copy()

import time
from sklearn.decomposition import KernelPCA
kernel_pca = KernelPCA(kernel="rbf", gamma=10, fit_inverse_transform=True, alpha=0.1, n_components=None)

start = time.time()
X_kpca_reduced = kernel_pca.fit_transform(X_train2)
end = time.time()
execution_time = end - start
execution_time

X_kpca_reduced.shape

score_kernel_pca = kernel_pca.transform(X_test)

plt.plot(kernel_pca.eigenvalues_)

plt.title("Principal component and their eigenvalues")
plt.xlabel("nth principal component")
plt.ylabel("eigenvalue magnitude")
plt.show()

plt.scatter(score_kernel_pca[:,0],score_kernel_pca[:,1] ,c=y_test,cmap='viridis')
plt.title("Projection onto PCs (kernel)")
plt.xlabel("1st principal component")
plt.ylabel("2nd principal component")
plt.show()

"""Let's test it with RandomForestClassifier on Reduced Data"""

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=100, random_state = 42)
t0 = time.time()
rfc.fit(X_kpca_reduced, y_train)
t1 = time.time()

t1-t0

X_test_reduced = kernel_pca.transform(X_test)
rfc.score(X_test_reduced, y_test)

"""Let's test it with SoftmaxClassifier on Reduced Data"""

log_clf2 = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', random_state = 42)
t0 = time.time()
log_clf2.fit(X_kpca_reduced, y_train)
t1 = time.time()

t1-t0

log_clf2.score(X_test_reduced, y_test)