{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Convolutional Networks part. 2"
      ],
      "metadata": {
        "id": "jA35jww5f6gd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGlS5kaqf40d",
        "outputId": "80012638-e1f6-499b-fa0d-a0fe2410ab11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 11s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 32, 32, 32)        128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 32, 32, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               524544    \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 256)               1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               25700     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 840068 (3.20 MB)\n",
            "Trainable params: 838660 (3.20 MB)\n",
            "Non-trainable params: 1408 (5.50 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "352/352 [==============================] - 387s 1s/step - loss: 4.2025 - accuracy: 0.0942 - val_loss: 5.0328 - val_accuracy: 0.0488\n",
            "Epoch 2/10\n",
            "352/352 [==============================] - 382s 1s/step - loss: 3.2911 - accuracy: 0.2051 - val_loss: 2.8763 - val_accuracy: 0.2792\n",
            "Epoch 3/10\n",
            "352/352 [==============================] - 384s 1s/step - loss: 2.8576 - accuracy: 0.2812 - val_loss: 2.5579 - val_accuracy: 0.3478\n",
            "Epoch 4/10\n",
            "352/352 [==============================] - 378s 1s/step - loss: 2.5821 - accuracy: 0.3367 - val_loss: 2.5769 - val_accuracy: 0.3514\n",
            "Epoch 5/10\n",
            "352/352 [==============================] - 381s 1s/step - loss: 2.4067 - accuracy: 0.3720 - val_loss: 2.4232 - val_accuracy: 0.3838\n",
            "Epoch 6/10\n",
            "352/352 [==============================] - 375s 1s/step - loss: 2.2685 - accuracy: 0.4007 - val_loss: 2.1100 - val_accuracy: 0.4396\n",
            "Epoch 7/10\n",
            "352/352 [==============================] - 379s 1s/step - loss: 2.1493 - accuracy: 0.4274 - val_loss: 2.0084 - val_accuracy: 0.4604\n",
            "Epoch 8/10\n",
            "352/352 [==============================] - 377s 1s/step - loss: 2.0495 - accuracy: 0.4441 - val_loss: 1.9550 - val_accuracy: 0.4772\n",
            "Epoch 9/10\n",
            "352/352 [==============================] - 379s 1s/step - loss: 1.9795 - accuracy: 0.4624 - val_loss: 1.9453 - val_accuracy: 0.4772\n",
            "Epoch 10/10\n",
            "352/352 [==============================] - 369s 1s/step - loss: 1.8890 - accuracy: 0.4816 - val_loss: 1.8610 - val_accuracy: 0.4964\n",
            "313/313 [==============================] - 22s 72ms/step - loss: 1.8159 - accuracy: 0.5085\n",
            "Test accuracy: 0.5084999799728394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "train_labels = to_categorical(train_labels, num_classes=100)\n",
        "test_labels = to_categorical(test_labels, num_classes=100)\n",
        "\n",
        "# Define a more complex CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.4),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"cifar100_complex_cnn_model.h5\")"
      ]
    }
  ]
}