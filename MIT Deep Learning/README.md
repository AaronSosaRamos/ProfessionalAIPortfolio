# MIT Deep Learning Basic Course

This section provides a detailed overview of the topics covered in the MIT Deep Learning Basic Course. Each topic builds a strong foundation in deep learning, covering both theoretical concepts and practical implementations.

## Deep Learning Basics
- **Introduction to Deep Learning Concepts and Terminologies:** 
  - Provides an overview of key concepts such as neurons, layers, activations, and the terminology used in deep learning.
  - Introduces the foundational ideas behind neural networks and their applications.

## Introduction to DL
- **Neural Networks:** 
  - Explains the structure and functioning of neural networks.
- **Binary Classification:** 
  - Describes binary classification tasks and how neural networks can be used to solve them.
- **Logistic Regression:** 
  - Covers logistic regression as a method for binary classification, including its implementation and optimization.

## Computation Graph, Vectorization, Shallow Neural Networks
- **Building Computation Graphs:** 
  - Teaches how to construct computation graphs to represent mathematical operations in neural networks.
- **Vectorized Implementations:** 
  - Introduces vectorization techniques to optimize computations and improve efficiency.
- **Shallow Neural Networks from Scratch:** 
  - Guides through building and training shallow neural networks from scratch.
- **Random Initialization:** 
  - Discusses the importance of proper weight initialization for training stability and convergence.

## Deep L-layer Neural Networks
- **Implementing Deep Neural Networks:** 
  - Covers the construction and training of deep neural networks with multiple layers.
- **Fashion MNIST Dataset:** 
  - Uses the Fashion MNIST dataset as a practical example to demonstrate the implementation of deep networks.

## Calculus for Deep Learning (Limits and Intermediate Value Theorem)
- **Fundamental Calculus Concepts:** 
  - Explains limits and the Intermediate Value Theorem.
  - Discusses their relevance to understanding gradient descent and optimization in deep learning.

## Calculus for Deep Learning (Differentiation and Derivatives)
- **Techniques for Differentiation:** 
  - Covers the basics of differentiation, including the chain rule and partial derivatives.
  - Explains how these techniques are applied in backpropagation for neural networks.

## Math with Python and Numpy
- **Arrays:** 
  - Introduces arrays and their manipulation using Python and Numpy.
- **Normalization Techniques:** 
  - Covers L1 and L2 normalization, min-max scaling, and other normalization techniques to prepare data for neural networks.

## Logistic Neural Networks
- **Logistic Regression Models:** 
  - Provides a deep dive into logistic regression models for classification tasks.
- **Neural Network Implementation for Logistic Tasks:** 
  - Demonstrates how to implement logistic regression using neural networks.

## Shallow Neural Networks
- **Architecture of Shallow Neural Networks:** 
  - Explains the structure and design of shallow neural networks.
- **Training Shallow Neural Networks:** 
  - Covers the training process, including forward and backward propagation.

## Gradient Descent Types
- **Batch Gradient Descent:** 
  - Discusses the standard approach of batch gradient descent.
- **Stochastic Gradient Descent:** 
  - Explains stochastic gradient descent and its benefits.
- **Mini-Batch Gradient Descent:** 
  - Combines the advantages of both batch and stochastic gradient descent.

## Exponentially Weighted Averages, Momentum, RMSProp
- **Accelerating Gradient Descent:** 
  - Introduces techniques like exponentially weighted averages and momentum to speed up gradient descent.
- **RMSProp:** 
  - Discusses RMSProp as an adaptive learning rate method to improve convergence.

## Adam, Learning Rate Schedule, GridCV
- **Adam Optimizer:** 
  - Explains the Adam optimization algorithm and its advantages.
- **Learning Rate Schedules:** 
  - Covers various strategies for adjusting the learning rate during training.
- **Grid Search Cross Validation (GridCV):** 
  - Introduces GridCV for hyperparameter tuning to find the best model parameters.

## Pandas, Caviar, Batch Normalization
- **Data Manipulation with Pandas:** 
  - Demonstrates how to use Pandas for data manipulation and analysis.
- **Caviar Dataset:** 
  - Uses the Caviar dataset as an example to illustrate data handling and preprocessing.
- **Batch Normalization:** 
  - Explains batch normalization and its role in training deep neural networks.

## Softmax
- **Softmax Activation Function:** 
  - Covers the softmax function for multiclass classification.
  - Discusses its implementation and usage in neural networks.

## Gradient Checking
- **Verifying Backpropagation Implementations:** 
  - Introduces gradient checking as a method to verify the correctness of backpropagation implementations.
  - Provides practical examples to demonstrate gradient checking techniques.